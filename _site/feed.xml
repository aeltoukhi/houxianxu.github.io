<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xianxu Hou blog</title>
    <description>From Coal Mining to Data Mining</description>
    <link>http://houxianxu.github.io/</link>
    <atom:link href="http://houxianxu.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 13 Sep 2016 11:53:58 +0800</pubDate>
    <lastBuildDate>Tue, 13 Sep 2016 11:53:58 +0800</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>CNN Face Dectection</title>
        <description>&lt;p&gt;The post records some notes for CNN Face Dectection project in my PhD in the University of Nottingham.&lt;/p&gt;

&lt;h3 id=&quot;note-1-make-image-square-and-cropsplit-it-into-subimages&quot;&gt;Note 1: Make image square and crop/split it into sub_images&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Make image square&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to use Convolutional Neural Network that (mostly) requires the input image square, i.e. of shape (3, N, N), I need to make the height equals to width. There are 3 ways coming into my mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stretch the image to square, &lt;strong&gt;not good&lt;/strong&gt; because the face could be stretched.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;stretch_to_square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Crop the image to square, usually set the cropped size as the smaller one of width and height. &lt;strong&gt;Not good&lt;/strong&gt; because information could be lost&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;crop_to_square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# landscape&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# portrait&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Padded the image with zeros to square, &lt;strong&gt;better solution&lt;/strong&gt;. However we need to store the padded size in order to convert from padded image coordinates to original image coordinates. We can define a new class SuperImage to achieve this.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;padding_to_square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookkeeping&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up_scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;super_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SuperImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'constant'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up_scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# up scale the image, more padding if needed&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up_scale&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'constant'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;super_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SuperImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'constant'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up_scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up_scale&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'constant'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;super_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SuperImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookkeeping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;super_image&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_frame&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SuperImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Store the current position of a super_image (after padding) &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        - image: an array, represent the sub image
        - y: int to represent the top coordinate in the parent image
        - x: int to represent the left coordinate in the parent image
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'SuperImage'&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sup_y0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sup_x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;old_x_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; convert (y, x) in the super image to the parent / old image coordinate system &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sup_y0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sup_x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Split image into sub_images&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For simplicity (because simple is good), I use sliding window to split images. I design to overlap the sub_images to make sure any two continuous pixels can appear at least one sub_image. Specifically, if I want to get n*n sub_images, then set &lt;code class=&quot;highlighter-rouge&quot;&gt;stride = int(1.0/n * height)&lt;/code&gt;, and sub_image size &lt;code class=&quot;highlighter-rouge&quot;&gt;sub_size = 2 * stride&lt;/code&gt; to overlap half of the sub_images. If the size of remains are not enough to form a sub_image, we can add zero_padding or just throw them away.
In addition, we can use SubImage Class to store the information to convert sub_image coordinates back to original image.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;split_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_to_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; split to n*n SubImages based on window sliding &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# make the image square&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sub_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;square_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding_to_square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;square_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride_ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sub_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;padding_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;square_image&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_to_fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# decide how much padding need to make sure the sliding &quot;fit&quot; across input neatly&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# it is like the zero_padding in the convolutional layer&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;remain_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remain_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# need zero_padding, note: here should include 0 to make sure every image has the same number of sub images&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remain_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# may not be even&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remain_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;padding_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'constant'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;padding_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;square_image&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'pad'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_after&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pad_image_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_image_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_image_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_image_size&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# not pad to fit &lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;sub_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;# we need the original not the padded image in above when pad_to_fit&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;sub_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SubImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_before&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# it is a little ugly, but it works&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;sub_images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_images&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SubImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Store the current position of a sub_image &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        - image: an array, represent the sub image
        - y: int to represent the top coordinate in the parent image
        - x: int to represent the left coordinate in the parent image
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'SubImage'&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sup_y0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sup_x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;old_x_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; convert (y, x) in the subimage to the parent image coordinate system &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sup_y0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sup_x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;** Convert from subimage to original image&lt;/p&gt;

&lt;p&gt;The process is a little tricky. Because the subimages are actually cropped from padded image, the coordinates of subimages in the coordinate of original image can be less than 0, and also bigger than the original image size (see following figure). So we should make sure the details right when convert subimage information (e.g. human face bounding box) to original image coordinates.
&lt;img src=&quot;http://houxianxu.github.io/images/CNNFace/1.png&quot; alt=&quot;padded and subimage&quot; title=&quot;padded and subimage&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Following code illustrate how to convert a subimage heatmap to the corresponding position in the coordinates of the original image&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pad_subheatmap_to_old_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_heatmap_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Zero-pad sub_heatmap to the size of the old image size of which the sub_images cropped from &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;padded_heatmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded_square&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sub_image_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_heatmap_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;padded_square_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_square&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# get coordinate in old/super image of the origin point in the heatmap &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_start_old&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_start_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_x_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_end_old&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_end_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_start_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_start_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_image_size&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_start_sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_start_sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_end_sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_end_sub&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_image_size&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Here is a little tricky, please refer to image_misc.split_image&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Note, there are two images. One is the cropped sub image and the old image which the subimage cropped from&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_start_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_start_sub&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_start_old&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_start_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_start_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x_start_sub&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_start_old&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x_start_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_end_old&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_square_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_end_sub&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_image_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_end_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_square_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_end_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_square_size&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_end_old&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_square_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x_end_sub&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_image_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_end_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_square_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x_end_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_square_size&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;padded_heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_start_old&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_end_old&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_start_old&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_end_old&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_heatmap_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_start_sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_end_sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_start_sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_end_sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padded_heatmap&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Mon, 07 Dec 2015 00:00:00 +0800</pubDate>
        <link>http://houxianxu.github.io/2015/12/07/CNN-Face-Detection/</link>
        <guid isPermaLink="true">http://houxianxu.github.io/2015/12/07/CNN-Face-Detection/</guid>
        
        
      </item>
    
      <item>
        <title>Interpretation of matrix</title>
        <description>&lt;p&gt;When I study and implement machine learning algorithm, it is crucial and tricky to use matrix-matrix multiplication (which generalizes all matrix-vector and vector-vector) to speed up algorithms. However it is difficult to interpret the vectorized expressions, which needs strong linear algebra background. This post summarizes some basic concept in linear algebra and focuses more on the interpretation, which could be very helpful for us to understand some machine learning algorithms such as Neural Networks (just a chain of matrix-matrix multiplication).&lt;/p&gt;

&lt;h2 id=&quot;n-linear-equation-with-n-unknowns&quot;&gt;1. N linear equation with n unknowns&lt;/h2&gt;

&lt;p&gt;Example with N = 2 (2-dimension):
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{cases}2x - y = 0\\-x + 2y = 3\end{cases} = \begin{bmatrix}2 &amp; -1 \\-1 &amp; 2 \end{bmatrix} \begin{bmatrix}x \\y \end{bmatrix} = \begin{bmatrix}0 \\3 \end{bmatrix} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;row-picture&quot;&gt;Row picture&lt;/h4&gt;
&lt;p&gt;This is the way we often interpret the two equations above: just &lt;strong&gt;two line in 2-D space&lt;/strong&gt;, and the solution is the point lies on both lines.&lt;/p&gt;

&lt;!-- ![Plot of Two equations](/images/linearAlgbra/1.png &quot;Row picture&quot;) --&gt;
&lt;center&gt;&lt;img src=&quot;/images/linearAlgbra/1.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;h4 id=&quot;column-picture---linear-combination-of-columns&quot;&gt;Column picture - linear combination of columns&lt;/h4&gt;
&lt;p&gt;Follow the column we can rewrite the equations above as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x \begin{bmatrix}2 \\-1 \end{bmatrix}  + y \begin{bmatrix}-1 \\ 2 \end{bmatrix} = \begin{bmatrix}0 \\ 3 \end{bmatrix}&lt;/script&gt;

&lt;p&gt;We can interpret the above equation as &lt;strong&gt;linear combination of columns&lt;/strong&gt; which are vectors in 2-D, and the &lt;strong&gt;+&lt;/strong&gt; is overloaded for 2-D vector addition, as compared with scalar addition in row picture interpretation. The geometry is shown below.&lt;/p&gt;

&lt;!-- ![Plot of Two equations](/images/linearAlgbra/2.png &quot;column picture&quot;) --&gt;
&lt;center&gt;&lt;img src=&quot;/images/linearAlgbra/2.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;p&gt;When considering high dimension problem (say n = 10, i.e., 10 linear equation with n unknowns), it is not easy to imagine n-D space from Row Picture. However from Column Picture, the result is just the linear combination of 10 vectors.&lt;/p&gt;

&lt;h2 id=&quot;matrix-multiplication-as-linear-combination&quot;&gt;2. Matrix multiplication as linear combination&lt;/h2&gt;
&lt;p&gt;Usually we do matrix multiplication is to get the result cell as the dot product of a row in the first matrix with a column in the second matrix. However there is a very good interpretation from linear combination aspect, which is a core concept in linear algebra.&lt;/p&gt;

&lt;h4 id=&quot;linear-combination-of-columns-of-matrix&quot;&gt;2.1 Linear combination of columns of matrix&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}1 &amp; 2 &amp; 3\\ 4&amp;5&amp;6\\7&amp;8&amp;9 \end{bmatrix} \begin{bmatrix}a \\b \\c\end{bmatrix} 
= a \begin{bmatrix}1 \\ 4\\7 \end{bmatrix} + b \begin{bmatrix}2 \\ 5\\8 \end{bmatrix} + c \begin{bmatrix}3 \\ 6\\9 \end{bmatrix} = \begin{bmatrix}1a + 2b + 3c \\ 4a + 5b+6c\\7a+8b+9c \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Representing the columns of matrix by colorful boxes will help visualize this as follows: (the picture is from &lt;a href=&quot;http://eli.thegreenplace.net/2015/visualizing-matrix-multiplication-as-a-linear-combination/&quot;&gt;Eli Bendersky&lt;/a&gt;)&lt;/p&gt;

&lt;!-- ![matrix-vector](/images/linearAlgbra/3.png) --&gt;
&lt;center&gt;&lt;img src=&quot;/images/linearAlgbra/3.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;p&gt;For matrix multiply a column vector, the result is a column vector which is the linear combination of the columns of the matrix and the coefficients are the second vector. This idea can also be generalized to Matrix-Matrix multiplication, i.e., the columns of the result matrix is the first matrix multiply each column (vector) in the second matrix respectively. The following picture shows the idea.&lt;/p&gt;

&lt;!-- ![matrix-matrix](/images/linearAlgbra/4.png &quot;matrix-matrix&quot;) --&gt;
&lt;center&gt;&lt;img src=&quot;/images/linearAlgbra/4.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;h4 id=&quot;linear-combination-of-rows-of-matrix&quot;&gt;2.2 Linear combination of rows of matrix&lt;/h4&gt;
&lt;p&gt;Similarly we can view the matrix as different rows. 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}a &amp;b &amp;c\end{bmatrix}  \begin{bmatrix}1 &amp; 2 &amp; 3\\ 4&amp;5&amp;6\\7&amp;8&amp;9 \end{bmatrix} 
= a \begin{bmatrix}1 &amp; 2 &amp; 3 \end{bmatrix} + b \begin{bmatrix}4&amp;5&amp;6 \end{bmatrix} + c \begin{bmatrix}7&amp;8&amp;9 \end{bmatrix} %]]&gt;&lt;/script&gt;
The above equation can be represented as follows:&lt;/p&gt;

&lt;!-- ![vector-matrix](/images/linearAlgbra/5.png &quot;vector-matrix&quot;) --&gt;
&lt;center&gt;&lt;img src=&quot;/images/linearAlgbra/5.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;p&gt;For matrix-matrix multiplication, the rows of the result matrix is each row (vector) in first matrix multiply the second matrix. The idea can be represented graphically following:&lt;/p&gt;

&lt;!-- ![matrix-matrix](/images/linearAlgbra/6.png &quot;matrix-matrix&quot;) --&gt;
&lt;center&gt;&lt;img src=&quot;/images/linearAlgbra/6.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;h4 id=&quot;column-row-multiplication&quot;&gt;2.3 Column-row multiplication&lt;/h4&gt;

&lt;p&gt;There is another interpretation of matrix multiplication from &lt;script type=&quot;math/tex&quot;&gt;column * row&lt;/script&gt; view.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}a \\ b \\ c \end{bmatrix} \begin{bmatrix}x &amp; y &amp; z \end{bmatrix} = \begin{bmatrix}ax&amp;ay&amp;az \\ bx&amp;by&amp;bz \\ cx&amp;cy&amp;cz \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The above result is a 3 by 3 matrix. And if we two matrix are m by n and n by p, the shape of the result matrix is m by p and the result is the sum of all the matrix (m by p) computed by all the &lt;script type=&quot;math/tex&quot;&gt;n^{th}&lt;/script&gt; column in the first matrix and &lt;script type=&quot;math/tex&quot;&gt;n^{th}&lt;/script&gt; row in the second matrix.&lt;/p&gt;

&lt;h4 id=&quot;block-matrix-multiplication&quot;&gt;2.4 Block matrix multiplication&lt;/h4&gt;
&lt;p&gt;There is even another amazing interpretation of matrix multiplication. It is often convenient to partition a matrix &lt;strong&gt;A&lt;/strong&gt; into smaller matrices called blocks. Then we can treat the blocks as matrix entries when do matrix multiplication.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
AB = \left[
\begin{array}{cc} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{array} \right]\cdot
\left[
\begin{array}{cc} b_{11} &amp; b_{12} \\ b_{21} &amp; b_{22} \end{array} \right]
= 
\left[
\begin{array}{cc} a_{11}b_{11}+a_{12}b_{21} &amp; a_{11}b_{12}+a_{12}b_{22} \\ a_{21}b_{11}+a_{22}b_{21} &amp; a_{22}b_{12}+a_{22}b_{22} \end{array} \right] %]]&gt;&lt;/script&gt;

&lt;p&gt;which is equal to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
AB = \left[
\begin{array}{c|c} A_{11} &amp; A_{12} \\\hline A_{21} &amp; A_{22} \end{array} \right]\cdot
\left[
\begin{array}{c|c} B_{11} &amp; B_{12} \\\hline B_{21} &amp; B_{22} \end{array} \right]
= 
\left[
\begin{array}{c|c} A_{11}B_{11}+A_{12}B_{21} &amp; A_{11}B_{12}+A_{12}B_{22} \\\hline A_{21}B_{11}+A_{22}B_{21} &amp; A_{22}B_{12}+A_{22}B_{22} \end{array} \right] %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;elimination-matrices&quot;&gt;2.5 Elimination matrices&lt;/h4&gt;

&lt;p&gt;The linear combination interpretation of matrix multiplication is very useful for us to understand matrix transformation. Especially when we do row operation, we can achieve elimination to solve a system of linear equations. Take the following matrix multiplication for example AX=B, we want to choose the first matrix &lt;strong&gt;A&lt;/strong&gt; in order to transform matrix &lt;strong&gt;X&lt;/strong&gt; to &lt;strong&gt;B&lt;/strong&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}a &amp;b &amp;c \\ d&amp;e&amp;f \\ g&amp;h&amp;i \end{bmatrix} \begin{bmatrix}1 &amp; 2 &amp; 3\\ 4&amp;5&amp;6\\7&amp;8&amp;9 \end{bmatrix} = \begin{bmatrix}1 &amp; 2 &amp; 3\\ 0&amp;-3&amp;-6\\7&amp;8&amp;9 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Recall that the row in result matrix the row in &lt;strong&gt;A&lt;/strong&gt; multiply &lt;strong&gt;X&lt;/strong&gt;, which is the linear combination of rows of &lt;strong&gt;X&lt;/strong&gt;. Because the first and third row is the same, so the first and third row in A should be &lt;script type=&quot;math/tex&quot;&gt;[1 \: 0 \: 0]&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;[0 \: 0 \: 1]&lt;/script&gt; and the second row in &lt;strong&gt;B&lt;/strong&gt; is the second row minus 4 times first row in &lt;strong&gt;A&lt;/strong&gt;, i.e., &lt;script type=&quot;math/tex&quot;&gt;[row2 - 4*row1]&lt;/script&gt;. So the second row in &lt;strong&gt;A&lt;/strong&gt; should be &lt;script type=&quot;math/tex&quot;&gt;[-4 \: 1 \: 0]&lt;/script&gt;. Put all together A = &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}1 &amp; 0 &amp; 0\\ -4&amp;1&amp;0\\0&amp;0&amp;1 \end{bmatrix} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;permutation-of-matrix&quot;&gt;2.6 Permutation of matrix&lt;/h4&gt;
&lt;p&gt;Exchange the two rows in a matrix &lt;strong&gt;A&lt;/strong&gt;, we just need to multiply some matrix on the left as shown as follows. For example in the result matrix, the first row is the linear combination of the rows in the second matrix with respect to first row in the first matrix. What we want is the second row, so the second cell in the first matrix should be 1, and first cell should be 0, which has no contribution to the first row in the result matrix.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix}a &amp; b \\ c &amp; d \end{bmatrix} = \begin{bmatrix}c &amp; d \\ a &amp; b \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;If want to exchange the columns, we just need to do column operation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}a &amp; b \\ c &amp; d \end{bmatrix} \begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}  = \begin{bmatrix}b &amp; a \\ d &amp; c \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;In short, if we want to do column operations the matrix multiplies on the right, and to do row operations, it multiplies on the left.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;##3 Inverse or non-singular matrix
Suppose &lt;strong&gt;A&lt;/strong&gt; is square matrix, and if &lt;script type=&quot;math/tex&quot;&gt;A A^{-1} = I&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt; is identity matrix), then matrix A is invertible and the inverse matrix is &lt;script type=&quot;math/tex&quot;&gt;A^{-1}&lt;/script&gt;. We can see whether inverse matrix is a property for a given matrix, and not all matrices have inverse matrix. One simple way to determine whether you can find a vector &lt;strong&gt;x&lt;/strong&gt; != &lt;strong&gt;0&lt;/strong&gt; with &lt;strong&gt;Ax = b&lt;/strong&gt;, and if you cannot find a &lt;strong&gt;x&lt;/strong&gt;, then A has inverse matrix, otherwise not. For example,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}1 &amp; 3 \\ 2 &amp; 6 \end{bmatrix} \begin{bmatrix}3  \\ -1 \end{bmatrix} = \begin{bmatrix}0\\0 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;There couldn’t be an inverse for the first matrix above. We can think that the first and second column (vector) are in same direction, the linear combination of them can not be &lt;strong&gt;0&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If matrix A does has inverse matrix, then Guass-Jordan elimination can solve it: &lt;script type=&quot;math/tex&quot;&gt;E[A I] = [I A^{-1}]&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;EA = I&lt;/script&gt;. You interpret the equation as block matrix multiplication. We can use inverse matrix to factorize matrix as two matrix multiplication. First use elimination (or elemental) matrix &lt;strong&gt;E&lt;/strong&gt; to transform &lt;strong&gt;A&lt;/strong&gt; into &lt;strong&gt;U&lt;/strong&gt;, i.e. &lt;script type=&quot;math/tex&quot;&gt;EA = U&lt;/script&gt;, then solve A as &lt;script type=&quot;math/tex&quot;&gt;A = E^{-1}U = LU&lt;/script&gt;. The factors &lt;strong&gt;L&lt;/strong&gt; and &lt;strong&gt;U&lt;/strong&gt; are triangular matrices. Because of &lt;strong&gt;U&lt;/strong&gt; is the result of elimination, so it should be a upper triangular matrix. For &lt;strong&gt;L&lt;/strong&gt;, we can use Guass-Jordan elimination &lt;script type=&quot;math/tex&quot;&gt;E[A I] = [I A^{-1}]&lt;/script&gt; to compute the &lt;strong&gt;L&lt;/strong&gt;, because the we just do elimination of &lt;strong&gt;I&lt;/strong&gt; with &lt;strong&gt;E&lt;/strong&gt;, so the cell values in the upper bound are all zero, so &lt;strong&gt;L&lt;/strong&gt; is a lower triangular matrix. If we need to exchange rows, all we need is to multiply a permutation matrix on the left: &lt;script type=&quot;math/tex&quot;&gt;PA = E^{-1}U = LU&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Another property of invertible square matrix is that you can exchange transposing and inversing for a singular matrix. &lt;script type=&quot;math/tex&quot;&gt;(A^{-1})^T \: A^T = I&lt;/script&gt;, i.e., &lt;script type=&quot;math/tex&quot;&gt;(A^{-1})^T = (A^T)^{-1}&lt;/script&gt;. More more interesting thing is that when a matrix multiply its transpose, we get a symmetry matrix. &lt;script type=&quot;math/tex&quot;&gt;(A A^{T})^T = (A^T)^T A^T = AA^T&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;vector-spaces&quot;&gt;3. Vector Spaces&lt;/h2&gt;
&lt;p&gt;Vector space means the “space” of vectors, which should satisfy some rules, i.e., we can multiply any vector v by any scalar c in that space, that’s to say they can produce &lt;strong&gt;linear combination&lt;/strong&gt;. For example, &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^2&lt;/script&gt; space contains all the real vectors with 2 components and it represents x-y plane, and &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^2&lt;/script&gt; space contains all the real vectors with 2 components and it represents x-y plane, and &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^3&lt;/script&gt; space contains all the real vectors with 3 components and it represents x-y-z 3-d space.&lt;/p&gt;

&lt;p&gt;Subspace is a vector space which contains some or all the vectors from another vector space. Subspace should be satisfy the definition of space (linear combination) and it is based on another vector space. For instance, there are 4 subspace of &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^3&lt;/script&gt;: &lt;strong&gt;Z&lt;/strong&gt; – the single vector (0 0, 0); (&lt;strong&gt;L&lt;/strong&gt;) – any line through (0, 0, 0); (&lt;strong&gt;P&lt;/strong&gt;) – any plane through(0, 0, 0); &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^3&lt;/script&gt; – the whole space.&lt;/p&gt;

&lt;p&gt;In return, we can use some vectors or a matrix to construct vector space because all we need is linear combination. Given a matrix, the linear combination of all the columns of matrix from a space, which is called &lt;strong&gt;column space&lt;/strong&gt;. For example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
A = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4 \\ 5&amp;6 \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The column vector of A is in &lt;script type=&quot;math/tex&quot;&gt;mathbb{R}^3&lt;/script&gt;, and all the combinations of columns form a subspace, which is plane through origin. And the intersection of two subspace is a subspace.&lt;/p&gt;

&lt;p&gt;##4. Interpret &lt;script type=&quot;math/tex&quot;&gt;Ax=b&lt;/script&gt; with vector space
 #### column space
We can interpret &lt;script type=&quot;math/tex&quot;&gt;Ax&lt;/script&gt; (&lt;strong&gt;A&lt;/strong&gt; is a matrix and &lt;strong&gt;x&lt;/strong&gt; is a vector) as linear combination of columns of matrix using vector x, i.e., which is the columns space define by matrix A. So we can view &lt;script type=&quot;math/tex&quot;&gt;Ax=b&lt;/script&gt; as finding the perfect linear combination of the columns to make it equal to vector &lt;strong&gt;b&lt;/strong&gt;, and the vector &lt;strong&gt;b&lt;/strong&gt; should be in the column space defined by matrix &lt;strong&gt;A&lt;/strong&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Ax = \begin{bmatrix}1 &amp; 5 &amp;6\\ 2 &amp; 6 &amp; 8\\ 3&amp;7&amp;10 \\4&amp;8&amp;12 \end{bmatrix} \begin{bmatrix}x \\ y \\ z \end{bmatrix} = b %]]&gt;&lt;/script&gt;

&lt;p&gt;Take above equation for example, not for all b we can find a solution. Because &lt;strong&gt;b&lt;/strong&gt; could be any vector in &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^4&lt;/script&gt; and the left hand side the combinations of 3 columns don’t fill the whole 4-D space. The fact is that there are a lot of vectors b are not the combinations of the 3 columns (column subspace which is inside &lt;script type=&quot;math/tex&quot;&gt;\\mathbb{R}^4&lt;/script&gt;). We can only solve the equation when &lt;strong&gt;b&lt;/strong&gt; is in the column space of &lt;strong&gt;A&lt;/strong&gt;, for example, &lt;script type=&quot;math/tex&quot;&gt;x = [1 \: 0 \: 0]^T&lt;/script&gt;, when &lt;script type=&quot;math/tex&quot;&gt;b = [1 \: 2 \: 3 \: 4]^T&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;null-space&quot;&gt;Null space&lt;/h4&gt;
&lt;p&gt;Particularly for equation &lt;script type=&quot;math/tex&quot;&gt;Ax=b=0&lt;/script&gt;, we can get a bunch of vectors &lt;strong&gt;x&lt;/strong&gt; as the solutions and this vectors can compose a subspace because &lt;script type=&quot;math/tex&quot;&gt;A(v + w) = Av + Aw = 0&lt;/script&gt;. Take example above (when b = 0), the solution is &lt;script type=&quot;math/tex&quot;&gt;c[1\:1\:-1]^T&lt;/script&gt; (c is constant), which is called &lt;strong&gt;Null space&lt;/strong&gt;. And if &lt;script type=&quot;math/tex&quot;&gt;b != 0&lt;/script&gt;, then the solution is not a subspace because &lt;strong&gt;0&lt;/strong&gt; is not in that bunch of vectors.&lt;/p&gt;

&lt;h2 id=&quot;compute-ax--b&quot;&gt;5. Compute &lt;script type=&quot;math/tex&quot;&gt;Ax = b&lt;/script&gt;&lt;/h2&gt;
&lt;h4 id=&quot;null-space-b--0&quot;&gt;Null space b = 0&lt;/h4&gt;
&lt;p&gt;We can do elimination for matrix A and get the matrix &lt;strong&gt;U&lt;/strong&gt;, and then continue simplifying the &lt;strong&gt;U&lt;/strong&gt; to get matrix &lt;strong&gt;R&lt;/strong&gt; which is reduced row echelon form and &lt;strong&gt;R&lt;/strong&gt; has the form of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}I&amp;F\\ O&amp;O\\\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt; is identity matrix and indicates pivot variables. In fact the particular solutions are the columns of matrix
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
N = {\begin{bmatrix}-F &amp;I\end{bmatrix}}^T %]]&gt;&lt;/script&gt;, the null space is the column space of N. This is because of following matrix block multiplication.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}I&amp;F\\ O&amp;O\\\end{bmatrix} {\begin{bmatrix}-F \\ I\end{bmatrix}} = O %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;b--0&quot;&gt;b != 0&lt;/h4&gt;
&lt;p&gt;First we should consider whether the equation has solution or not, as mentioned above, &lt;strong&gt;Ax = b&lt;/strong&gt; is solvable when b is column space of A, i.e., C(A). On the other hand, after finishing the elimination step, if the a combination of rows of A gives zero rows, the same combination of entries of b must give 0.&lt;/p&gt;

&lt;p&gt;If the equation does have solutions, we can use elimination to find a particular solution. As long as we get one particular solution, the complete solution is the particular solution plus the any vector in the null space of &lt;strong&gt;A&lt;/strong&gt;. that’s to say, &lt;script type=&quot;math/tex&quot;&gt;x = x_{particular} + x_{null}&lt;/script&gt;. The shape of the complete solution is similar to Null space, we can interpret the complete space as null space which is shifted by vector &lt;script type=&quot;math/tex&quot;&gt;x_{particular}&lt;/script&gt;. This is because:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Ax_p = b \: \: and \:\:Ax_n = 0 \:\:= \: A(x_p + x_b) = b&lt;/script&gt;

&lt;h4 id=&quot;solution-discussion----m-by-n-matrix-a-of-rank-r&quot;&gt;Solution discussion – m by n matrix A of rank r&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Full column rank, i.e., r = n&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;There are free variables&lt;/li&gt;
  &lt;li&gt;The null space is {&lt;strong&gt;0&lt;/strong&gt;}&lt;/li&gt;
  &lt;li&gt;Unique solution if it exists (0 or 1 solution)&lt;/li&gt;
  &lt;li&gt;The reduced row echelon form is &lt;script type=&quot;math/tex&quot;&gt;\begin{bmatrix}I\\ O\\\end{bmatrix}&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Full row rank, i.e., r = m&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It can be solved Ax=b for every b, because every row have a pivot and no zero rows.&lt;/li&gt;
  &lt;li&gt;There are n - r free variables and there are infinite solutions.&lt;/li&gt;
  &lt;li&gt;The reduced row echelon form is &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}I&amp;F\end{bmatrix} %]]&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Full column and row rank, i.e., r = m = n&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Invertible matrix of A&lt;/li&gt;
  &lt;li&gt;Unique solution&lt;/li&gt;
  &lt;li&gt;The reduced row echelon form is identity &lt;strong&gt;I&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Not full rank, i.e., r &amp;lt; m, and r &amp;lt; n&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;There are no solutions or infinite solutions&lt;/li&gt;
  &lt;li&gt;The reduced row echelon form is &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}I&amp;F\\0&amp;0\end{bmatrix} %]]&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;independent-span-basis-dimension&quot;&gt;6. Independent, span, basis dimension&lt;/h2&gt;
&lt;p&gt;Independent is used to describe the relation between vectors. Vectors &lt;script type=&quot;math/tex&quot;&gt;v_1, v_2, ..., v_n&lt;/script&gt; are Independent if no linear combination gives zero vector (except the zero combination), i.e., &lt;script type=&quot;math/tex&quot;&gt;c_1 v_1 + c_2 v_2 + ... + c_n v_n != 0&lt;/script&gt;. From vector space point of view, &lt;script type=&quot;math/tex&quot;&gt;v_1, v_2, ..., v_n&lt;/script&gt; are columns of matrix &lt;strong&gt;A&lt;/strong&gt;, they are independent if null space of A is zero vector and the rank r = n with no free variables, and they are independent if Ac = 0 for some non-zero vector c and rank &amp;lt; n with free variables.&lt;/p&gt;

&lt;p&gt;Vectors &lt;script type=&quot;math/tex&quot;&gt;v_1, v_2, ..., v_n&lt;/script&gt; &lt;strong&gt;span&lt;/strong&gt; a vector space means that the space contains all the linear combination of those vectors. They vectors could be independent or dependent.&lt;/p&gt;

&lt;p&gt;We are more interested in the vectors spanning a space are independent, which means the right number or minimal number of vectors to span a given space and we use &lt;strong&gt;basis&lt;/strong&gt; to indicate this idea. Basis for a vector space is segment of vectors with 2 properties: (1) They are independent; (2)They span a space.&lt;/p&gt;

&lt;p&gt;For a given space such as &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^4&lt;/script&gt;, every basis has the same number of vectors and the number is called &lt;strong&gt;dimension of the space&lt;/strong&gt;. So when putting all together, we get the conclusion the rank of a matrix &lt;strong&gt;A&lt;/strong&gt; == the number of pivot columns == dimension of the column space.&lt;/p&gt;

&lt;h2 id=&quot;orthogonal&quot;&gt;7. Orthogonal&lt;/h2&gt;
&lt;p&gt;Vector x is orthogonal to vector y, when &lt;script type=&quot;math/tex&quot;&gt;x^T y = 0&lt;/script&gt;
Subspace S is orthogonal to subspace T means: every vector in S is orthogonal to every vector in T. For every space, the row space is orthogonal to nullspace. Because of Ax = 0, so the linear combination of rows respecting to null space is 0, i.e. &lt;script type=&quot;math/tex&quot;&gt;\sum_i^m c_i \: row_i = 0&lt;/script&gt;
Moreover, nullspace and row space are orthogonal complements in &lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt; and nullspace contains all vectors perpendicular to the row space.&lt;/p&gt;

&lt;h2 id=&quot;solve-ax--b-when-there-are-no-solutions&quot;&gt;8. “Solve” &lt;script type=&quot;math/tex&quot;&gt;Ax = b&lt;/script&gt; when there are no solutions&lt;/h2&gt;
&lt;p&gt;We know that &lt;script type=&quot;math/tex&quot;&gt;Ax = b&lt;/script&gt; is only solvable when vector &lt;strong&gt;b&lt;/strong&gt; is in the column space of A. In practice, this equation is often unsolvable when A is a rectangular. Take m by n matrix (m  n)for example, there are more constrains or equations than unknown variables and there may be no solutions when some equations conflict each other. In other words, there is a lot of information about x here. One naive method is only using some of information (equations), however, there is no reason to say some equations are perfect and some are useless and we want to use all the information to get the best solution.&lt;/p&gt;

&lt;p&gt;When &lt;script type=&quot;math/tex&quot;&gt;Ax = b&lt;/script&gt; cannot be solved perfectly, what can we do? And can we do better? The reason that the equation is not solvable is because &lt;strong&gt;b&lt;/strong&gt; is not in the column space of &lt;strong&gt;A&lt;/strong&gt;, so we may be able to find a “closest” vector (say &lt;strong&gt;p&lt;/strong&gt;) to replace &lt;strong&gt;b&lt;/strong&gt; in the column space of &lt;strong&gt;A&lt;/strong&gt;, i.e., &lt;script type=&quot;math/tex&quot;&gt;A\hat{x} = p&lt;/script&gt;, and use &lt;script type=&quot;math/tex&quot;&gt;\hat{x}&lt;/script&gt; to estimate &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. The next problem is to define the “best” &lt;strong&gt;p&lt;/strong&gt;, and the projection of &lt;strong&gt;b&lt;/strong&gt; onto column space &lt;strong&gt;A&lt;/strong&gt; is used instead. And then we need to find a way to calculate the projected vector &lt;strong&gt;p&lt;/strong&gt; from &lt;strong&gt;b&lt;/strong&gt;.&lt;/p&gt;

&lt;!-- ![projection from vector to vector](/images/linearAlgbra/7.png) --&gt;
&lt;p&gt;&lt;img src=&quot;/images/linearAlgbra/7.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For simplicity, we first consider projection from vector to vector (see the left diagram above). We use &lt;strong&gt;p&lt;/strong&gt; to indicate the projection of &lt;strong&gt;b&lt;/strong&gt; onto &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt; is equal to some multiple of &lt;strong&gt;a&lt;/strong&gt;, i.e., &lt;script type=&quot;math/tex&quot;&gt;xa&lt;/script&gt;. According to is perpendicular to e, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^T e = a^T (b-xa) = a^Tb - xa^Ta = 0 \: = \: x = \frac{a^Tb}{a^Ta} \\
So \: p = xa = \frac{a^Tb}{a^Ta} a \\
We \: can \: rewrite \: as \: p = P b = \big(\frac{a a^T}{a^T a}\big) b&lt;/script&gt;

&lt;p&gt;Notice that &lt;strong&gt;P&lt;/strong&gt; is a n by n matrix if vector b and a have n elements and it is determined only by the vector &lt;strong&gt;a&lt;/strong&gt; which we want to project onto. P is called projection matrix and we can interpret it as a &lt;strong&gt;function&lt;/strong&gt; coming from vector &lt;strong&gt;a&lt;/strong&gt; to project another vector to itself. Additionally we can observe that &lt;script type=&quot;math/tex&quot;&gt;P^T = (\frac{a a^T}{a^T a})^T = P&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;PP = \frac{a a^T}{a^T a} \frac{a a^T}{a^T a} = \frac{a (a^T a) a^T} {(a^T a) (a^T a)} = P&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Next we consider projection from vector to space (see the right diagram above), the plane is the column space of A = [a1 a2], the error &lt;strong&gt;e = b - p&lt;/strong&gt; and it is perpendicular to &lt;strong&gt;A&lt;/strong&gt;. The projection &lt;strong&gt;b&lt;/strong&gt; to &lt;strong&gt;A&lt;/strong&gt; is &lt;script type=&quot;math/tex&quot;&gt;a_1 x_1 + x_2 x_2 = A x&lt;/script&gt; and our aim is to find &lt;strong&gt;x&lt;/strong&gt;. Because &lt;strong&gt;e = b - Ax&lt;/strong&gt; is perpendicular to the plane:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}a_1^T(b-Ax) = 0\\a_2^T(b-Ax) = 0\end{cases} \: = \begin{bmatrix}a_1^T \\a_2^T\end{bmatrix} \begin{bmatrix}b-Ax\end{bmatrix} = \begin{bmatrix}0 \\0\end{bmatrix} = A^T \begin{bmatrix}b-Ax\end{bmatrix} = 0 \\ = A^Tb - A^TA x = 0 \: = x = (A^TA)^{-1} A^T b \\ = p = A x = A (A^TA)^{-1} A^T b = P b&lt;/script&gt;

&lt;p&gt;From above equation we can interpret the (b-Ax) is in the null space of &lt;script type=&quot;math/tex&quot;&gt;A^T&lt;/script&gt;, and the (b-Ax) should be perpendicular to the row space of &lt;script type=&quot;math/tex&quot;&gt;A^T&lt;/script&gt; which is the column space of A, which is the plane defined by &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt;.
The matrix &lt;script type=&quot;math/tex&quot;&gt;P = A(A^TA){-1} A^T&lt;/script&gt; is called projection matrix, which can also be interpreted as a function to project a vector b onto the column space of &lt;strong&gt;A&lt;/strong&gt;. Moreover &lt;script type=&quot;math/tex&quot;&gt;P^T = P&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;PP = P&lt;/script&gt;. Here if A has independent columns then &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; is invertible. Here is a simple proof: suppose &lt;script type=&quot;math/tex&quot;&gt;A^TAx=0&lt;/script&gt;, then x must be 0 if &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; is invertible.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^TAx=0 \:= x^TA^TAx =0\:=(Ax)^T(Ax) = 0\:=Ax = 0=x=0&lt;/script&gt;

&lt;h2 id=&quot;determinant&quot;&gt;9. Determinant&lt;/h2&gt;
&lt;p&gt;The square matrix is relatively easy to deal with, and the determinant is a number that associates with a square matrix, &lt;script type=&quot;math/tex&quot;&gt;det\: A=\|A\|&lt;/script&gt;. Though this number can’t not tell you all the information of the matrix, it can tell you a lot of information. Following is some important properties of determinant.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Determinant of &lt;strong&gt;I&lt;/strong&gt; is 1&lt;/li&gt;
  &lt;li&gt;Exchange two rows of a matrix: reverse sign of determinant.&lt;/li&gt;
  &lt;li&gt;Linear combination of one row, and det (A+B) != det(A) + det(B)&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{vmatrix}ta &amp;tb \\c &amp;d \end{vmatrix} = t\begin{vmatrix}a &amp;b \\c &amp;d \end{vmatrix}\:\:\:\: and \:\:\: \: \begin{vmatrix}a + a'&amp;b'+b \\c &amp;d \end{vmatrix} = \begin{vmatrix}a &amp;b \\c &amp;d \end{vmatrix} + \begin{vmatrix}a' &amp;b' \\c &amp;d \end{vmatrix} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Two equal rows, then determinant is 0. We can get it by exchanging the same row.&lt;/li&gt;
  &lt;li&gt;Subtract l * row i from row k, and the determinant doesn’t change, i.e. elimination process.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{vmatrix}a &amp;b \\c-la &amp;d-lb \end{vmatrix} = \begin{vmatrix}a &amp;b \\c &amp;d \end{vmatrix} -l \begin{vmatrix}a &amp;b \\a &amp;b \end{vmatrix} = \begin{vmatrix}a &amp;b \\c-la &amp;d-lb \end{vmatrix} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Row of zeros, the determinant is equal to 0&lt;/li&gt;
  &lt;li&gt;Triangular matrix, the determinant is product of pivots, which is based on property 3.&lt;/li&gt;
  &lt;li&gt;det A = 0, when A is singular, and det A != 0 when A is invertible.&lt;/li&gt;
  &lt;li&gt;det(AB) = det(A)*det(B), and &lt;script type=&quot;math/tex&quot;&gt;det(A^{-1}) = \frac{1}{det(A)}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
det(A^T) = det(A) \:\: |A^T| = |A| &lt;= |U^TL^T| = |LU| &lt;= |U^T||L^T| = |L||U| %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cofactor of one entry
Cofactor of &lt;script type=&quot;math/tex&quot;&gt;a_{ij} = c_ij&lt;/script&gt; is the (+/-) determinant of n-1 matrix with row i and column j erased, when i+j is even choose +, and choose - when i+j is odd. 
Cofactor formula is: &lt;script type=&quot;math/tex&quot;&gt;det(A) = a_{i1}c_{i1} + a_{i2}c_{i2} + ... + a_{in}c_{in}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Another very good interpretation is that the determinant is the &lt;strong&gt;volume&lt;/strong&gt; of a box determined by row vectors.&lt;/p&gt;

&lt;h2 id=&quot;eigenvalue-and-eigenvector&quot;&gt;10. Eigenvalue and eigenvector&lt;/h2&gt;
&lt;p&gt;The result vector &lt;script type=&quot;math/tex&quot;&gt;Ax&lt;/script&gt; is parallel to x, and the vector &lt;strong&gt;x&lt;/strong&gt; is called eigenvector of square matrix A, i.e.,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Ax = \lambda x&lt;/script&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is called eigenvalue.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Ax = \lambda x \:\: = \:\: (A - \lambda I) x = 0&lt;/script&gt;

&lt;p&gt;Notice that &lt;script type=&quot;math/tex&quot;&gt;(A - \lambda I)&lt;/script&gt; should be singular, otherwise x must be 0. So the det&lt;script type=&quot;math/tex&quot;&gt;(A - \lambda I)&lt;/script&gt; = 0, and after finding the eigenvalues we can find the eigenvectors by computing null space. For most n by n matrix, there are n eigenvectors and eigenvalues. Some matrix just has one eigenvectors and eigenvalues of some matrix are complex number.&lt;/p&gt;

&lt;p&gt;Above we know how to compute the eigenvalues and eigenvectors, then how to use them. Suppose there are n independent eigenvectors of n by n matrix A, and put them in column matrix S.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
AS = A[x_1, x_2, ..., x_n] = [\lambda_1x_1, \lambda_2x_2, ..., \lambda_nx_n] = [x_1, x_2, ..., x_n] \begin{bmatrix}\lambda_1&amp;0&amp;\cdots&amp;0\\
0&amp;\lambda_2&amp;\cdots&amp;0\\
0&amp;0&amp;\ddots&amp;0\\
0&amp;0&amp;\cdots&amp;\lambda_n\end{bmatrix} = S\Lambda %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Lambda&lt;/script&gt; is called diagonal eigenvalue matrix&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AS = S\Lambda \:\: = \:\: S^{-1} AS = \Lambda \:\: =\:\: A = S\Lambda S^{-1}&lt;/script&gt;

&lt;p&gt;In fact, the eigenvectors and eigenvectors give a way what is going on inside a matrix and to understand the power of matrix. For example, &lt;script type=&quot;math/tex&quot;&gt;A^k - 0 \:\:as\:\: k - \infty&lt;/script&gt; if all &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\|\lambda_i\| &lt; 1 %]]&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Ax = \lambda x \:\: =\:\: A^2 x = \lambda Ax = {\lambda}^2 x \\
A^2 = S \Lambda S^{-1} S \Lambda S^{-1} = S {\Lambda}^2 S^{-1} \\
A^k = S {\Lambda}^k S^{-1}&lt;/script&gt;

&lt;p&gt;Base on the above equation, A is sure to have n independent eigenvectors and can be diagonalizable if all the eigenvalues &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; are different.&lt;/p&gt;

&lt;p&gt;We can use eigenvectors to solve following problem: &lt;script type=&quot;math/tex&quot;&gt;u_{k+1} = Au_k&lt;/script&gt;, start with a give vector &lt;script type=&quot;math/tex&quot;&gt;u_0&lt;/script&gt;, and we can see &lt;script type=&quot;math/tex&quot;&gt;u_k = A^k u_0&lt;/script&gt;.
To really solve above equation, we can first write &lt;script type=&quot;math/tex&quot;&gt;u_0&lt;/script&gt; as the linear combination of eigenvectors of matrix A. Then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_0 = c_1 x_1 + c_2 x_2 + ... + c_n x_n\\
= A u_0 = A(c_1 x_1 + c_2 x_2 + ... + c_n x_n) = c_1 \lambda_1 x_1 + c_2 \lambda_2 x_2 + ... + c_n \lambda_n x_n\\
= A^2 u_0 = A A u_0 = A (c_1 \lambda_1 x_1 + c_2 \lambda_2 x_2 + ... + c_n \lambda_n x_n) = c_1 {\lambda_1}^2 x_1 + c_2 {\lambda_2}^2 x_2 + ... + c_n {\lambda_n}^2 x_n\\
=A^k u_0 = c_1 {\lambda_1}^k x_1 + c_2 {\lambda_2}^k x_2 + ... + c_n {\lambda_n}^k x_n = S \Lambda c&lt;/script&gt;

&lt;p&gt;Using above idea to solve Fibonacci problem, [0, 1, 1, 2, 3, 5, 8, 13, …], then how to get &lt;script type=&quot;math/tex&quot;&gt;F_100&lt;/script&gt;. So we need to find how fast the sequence grows, which lies in the eigenvalues.&lt;/p&gt;

&lt;p&gt;We want the form &lt;script type=&quot;math/tex&quot;&gt;u_{k+1} = A u_k&lt;/script&gt; to use the matrix A. And we can construct the matrix as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{cases}F_{k+2} = F_{k+1} + F_k\\F_{k+1} = F_{k+1}\end{cases} \:= \: u_{k+1} = Au_k = \begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}u_k \:\: and \:\: u_{k} = \begin{bmatrix}F_{k+1}\\F_k\end{bmatrix}\\\\
A = \begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix} \:=\: |A-\lambda I| = {\lambda}^2 - \lambda - 1 = \lambda_1 =  \frac{1+\sqrt{5}}{2} \approx 1.618 \:\: \lambda_2 = \frac{1-\sqrt{5}}{2}  \approx  -0.618 \\
Eigenvectors \: are \: x_1 = [\lambda_1\:1]^T, \: x_2 = [\lambda_2 \:1]^T
u_0 = [1\: 0]^T = c_1 x_1 + c_2 x_2 \:=\: c_1 = 0.447 \: c_2 = -0.447\\
u_{100} = A^{100} u_0 = c_1 {\lambda_1}^{100} x_1 + c_2 {\lambda_2}^{100} x_2 = 0.447 (1.618)^{100} [1.618\:1]^T - 0.477 (-0.618)^{100} [-0.618\:1] \\
u_{100} \approx 0.447 (1.618)^{100} [1.618\:1]^T %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;first-order-differential-equation-du--dt--au&quot;&gt;11. First order differential equation &lt;script type=&quot;math/tex&quot;&gt;d_u / d_t = Au&lt;/script&gt;&lt;/h2&gt;
&lt;p&gt;We also arrange two or more equation into matrix form and try to solve it from matrix aspect. For example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{cases}\frac{du_1}{dt} = -u_1 + 2u_2\\\frac{du_2}{dt} = u_1-2u_2\end{cases} \:=\: \frac{du}{dt} = A\begin{bmatrix}u1\\u2\end{bmatrix} = \begin{bmatrix}-1 &amp;2\\1&amp;-2\end{bmatrix} \begin{bmatrix}u1\\u2\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can see from the above equation, u2 and u1 can be affected by each other and the relationship or all information, in fact, is the matrix A. So we maybe solve the differential equations as a single system. Usually this system is called Linear Dynamic System.&lt;/p&gt;

&lt;p&gt;We can use eigenvectors and eigenvalues of matrix A to solve above system. The result is very simple &lt;script type=&quot;math/tex&quot;&gt;u(t) = e^{tA}u(0)&lt;/script&gt;. The key point is the matrix exponential, which can be interpreted by Taylor Series.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
     \begin{split} 
e^{At} &amp;= I + At + \frac{(At)^2}{2!} + \frac{(At)^3}{3!} + ... + \frac{(At)^n}{n!} + ... \\ 
&amp;= SIS^{-1} + S \Lambda S^{-1} t + \frac{S {\Lambda}^2 S^{-1} t}{2!} + \frac{S {\Lambda}^n S^{-1} t}{n!} + ... \\
&amp;= S(I + \Lambda t + \frac{ {\Lambda}^2 t}{2!} + \frac{ {\Lambda}^n t}{n!} + ...)S^{-1} \\
&amp;= Se^{\Lambda t}S{-1}
	\end{split}
    \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the above equation, the S and &lt;script type=&quot;math/tex&quot;&gt;\Lambda&lt;/script&gt; are defined by eigenvectors and eigenvalues. Notice that the equation should based on the fact that there are n independent eigenvectors of matrix A, i.e., A can be diagonalized. Moreover the &lt;script type=&quot;math/tex&quot;&gt;e^{\Lambda t}&lt;/script&gt; can be easily computed as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
     \begin{split}
e^{\Lambda t} 
&amp;= I + \Lambda t + \frac{ {\Lambda}^2 t}{2!} + \frac{ {\Lambda}^n t}{n!} + ... \\
&amp;= I + {\begin{bmatrix}\lambda_1&amp;0&amp;\cdots&amp;0\\
0&amp;\lambda_2&amp;\cdots&amp;0\\
0&amp;0&amp;\ddots&amp;0\\
0&amp;0&amp;\cdots&amp;\lambda_n\end{bmatrix}}^1 + ... + \frac{1}{n!}{\begin{bmatrix}\lambda_1&amp;0&amp;\cdots&amp;0\\
0&amp;\lambda_2&amp;\cdots&amp;0\\
0&amp;0&amp;\ddots&amp;0\\
0&amp;0&amp;\cdots&amp;\lambda_n\end{bmatrix}}^n + ...\\
&amp;= \begin{bmatrix}e^{\lambda_1 t}&amp;0&amp;\cdots&amp;0\\
0&amp;e^{\lambda_2 t}&amp;\cdots&amp;0\\
0&amp;0&amp;\ddots&amp;0\\
0&amp;0&amp;\cdots&amp;e^{\lambda_n t}\end{bmatrix} \\
\end{split}
    \end{equation} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;symmetric-matrix-and-positive-definite&quot;&gt;12. Symmetric matrix and positive definite&lt;/h2&gt;
&lt;p&gt;Symmetric matrix is very special matrix and they are &lt;strong&gt;good&lt;/strong&gt; matrices: the eigenvalues are REAL and eigenvectors can be chosen PERPENDICULAR. For usual case &lt;script type=&quot;math/tex&quot;&gt;A = S \Lambda S^{-1}&lt;/script&gt;, for symmetric matrix &lt;script type=&quot;math/tex&quot;&gt;A = Q \Lambda Q^{-1} = Q \Lambda Q^T&lt;/script&gt;, and the matrix Q is the orthonormal eigenvectors matrix.&lt;/p&gt;

&lt;p&gt;In additional, &lt;script type=&quot;math/tex&quot;&gt;A = Q \Lambda Q^T = \sum_i^n \lambda_i q_i {q_i}^T&lt;/script&gt;, so every symmetric matrix is a combination of perpendicular projection matrices. This is because &lt;script type=&quot;math/tex&quot;&gt;q_i {q_i}^T = \frac{q_i {q_i}^T}{q_i^T q_i}&lt;/script&gt;, which is projection matrices.&lt;/p&gt;

&lt;p&gt;Then how about the sign of &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt;, and the eigenvalues decides between instability and stability in differential equations.
&lt;strong&gt;Positive definite&lt;/strong&gt; means that all the eigenvalues are positive. Positive semidefinite is called when eigenvectors are greater or equal to zero. Formally the Positive definite of matrix A is defined as the quadratic form is greater that zero, i.e., &lt;script type=&quot;math/tex&quot;&gt;x^TAx  0&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
x^TAx = \begin{bmatrix}x &amp;y\end{bmatrix} \begin{bmatrix}a &amp;b \\ c &amp;d\end{bmatrix} \begin{bmatrix}x \\y\end{bmatrix} = ax^2 + (b+c)xy + dy^2 %]]&gt;&lt;/script&gt;

&lt;p&gt;Another fact is that the signs of pivots are the same as signs of &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; and the number of positive pivot is equal to the number of positive eigenvalues.&lt;/p&gt;

&lt;p&gt;We can interpret the quadratic form &lt;script type=&quot;math/tex&quot;&gt;x^TAx = 1&lt;/script&gt; as tiled ellipse associated with symmetric matrix A.  &lt;script type=&quot;math/tex&quot;&gt;X^T \Lambda X = 1&lt;/script&gt; is a lined-up ellipse associated with eigenvalues matrix &lt;script type=&quot;math/tex&quot;&gt;Lambda&lt;/script&gt;, and we can use eigenvectors matrix Q to rotate the tiled ellipse to lined-up ellipse. Here is an example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
5x^2 + 8xy + 5y^2 = 1 \: =\: \begin{bmatrix}x&amp; y\end{bmatrix}A \begin{bmatrix}x\\y\end{bmatrix} = \begin{bmatrix}x&amp; y\end{bmatrix} \begin{bmatrix}5 &amp; 4\\4 &amp; 5\end{bmatrix} \begin{bmatrix}x\\y\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Use eigenvectors and eigenvalues to diagonalized &lt;script type=&quot;math/tex&quot;&gt;A = Q \Lambda Q^T&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}5 &amp; 4\\4 &amp; 5\end{bmatrix} = \frac{1}{\sqrt{2}} \begin{bmatrix}1 &amp; 1\\ 1&amp; -1\end{bmatrix} \begin{bmatrix}9 &amp; 0\\0 &amp; 1\end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix}1 &amp; 1\\1 &amp; -1\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;So &lt;script type=&quot;math/tex&quot;&gt;5x^2 + 8xy + 5y^2 =9(\frac{x+y}{\sqrt{2}})^2 + 1(\frac{x-y}{\sqrt{2}})^2 = 9X^2 + 1(Y)^2 = 1&lt;/script&gt;. We can see that the axes of the titled ellipse point are along the eigenvectors and the axis lengths are determined by eigenvalues, i.e. &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\sqrt{\lambda_i}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So in the xy system, the axes are along he eigenvectors of A. In the XY system, the axes are along the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;\Lambda&lt;/script&gt;. So if &lt;script type=&quot;math/tex&quot;&gt;A=Q \Lambda Q^T&lt;/script&gt; is positive definite, i.e., &lt;script type=&quot;math/tex&quot;&gt;\Lambda_i  0&lt;/script&gt;. The graph &lt;script type=&quot;math/tex&quot;&gt;x^TAx =1&lt;/script&gt; is an ellipse:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}x&amp; y\end{bmatrix} Q \Lambda Q^T \begin{bmatrix}x\\y\end{bmatrix} = \begin{bmatrix}X&amp; Y\end{bmatrix} \Lambda \begin{bmatrix}X\\Y\end{bmatrix} = \lambda_1 X^2 + \lambda_2 Y^2 = 1. %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A=Q \Lambda Q^T&lt;/script&gt; is called the &lt;strong&gt;principal axis theorem&lt;/strong&gt;, which can help displays the axes, or rotate to a lined-up position.&lt;/p&gt;

&lt;h2 id=&quot;singular-value-decomposition-svd&quot;&gt;Singular value decomposition (SVD)&lt;/h2&gt;
&lt;p&gt;For a full rank square matrix, we can diagonalize the matrix as &lt;script type=&quot;math/tex&quot;&gt;S^{-1}AS&lt;/script&gt;. However, if A is any m by n matrix with rank r, can we still diagonalize it?&lt;/p&gt;

&lt;p&gt;Based on the symmetric matrix, suppose A can be diagonalized as &lt;script type=&quot;math/tex&quot;&gt;A = U \Sigma V^T&lt;/script&gt;, U and V are orthonormal matrix, &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; is a diagonal matrix. The idea in fact is very simple, that’s we make A to be symmetric matrix.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^TA = V \Sigma^T U^T U \Sigma V^T = V \Sigma^2 V^T = Q_1 \Lambda Q_1^T\\
AA^T = U \Sigma V^T V \Sigma^T U^T = U \Sigma^2 U^T = Q_2 \Lambda Q_2^T&lt;/script&gt;

&lt;p&gt;Because &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt; are symmetric matrix, we easily get V, U and &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;. In details, &lt;script type=&quot;math/tex&quot;&gt;v_1, ... v_r&lt;/script&gt; are orthonormal singular vectors in row space of A, the &lt;script type=&quot;math/tex&quot;&gt;u_1, ..., u_r&lt;/script&gt; are orthonormal simple vectors in column space. We need n-r more v’s and m-r u’s from the null space N(A) and left null space &lt;script type=&quot;math/tex&quot;&gt;N(A^T)&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Vv_n=0v_m&lt;/script&gt;, and we set &lt;script type=&quot;math/tex&quot;&gt;\sigma_i =0&lt;/script&gt; when i  r.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
A[v_1, ..., v_r, ..., v_n] = [u_1, ... ,u_r, ..., u_m] \begin{bmatrix}\sigma_1&amp;0&amp;\cdots&amp;0\\
0&amp;\sigma_r&amp;\cdots&amp;0\\
0&amp;0&amp;\ddots&amp;0\\
0&amp;0&amp;\cdots&amp;0\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;reference-and-further-reading&quot;&gt;9. Reference and further reading&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/&quot;&gt;Introduction to Linear Algebra from MIT OpenCourseWare&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 18 May 2015 00:00:00 +0800</pubDate>
        <link>http://houxianxu.github.io/2015/05/18/matrix-multiply-interpretation/</link>
        <guid isPermaLink="true">http://houxianxu.github.io/2015/05/18/matrix-multiply-interpretation/</guid>
        
        
      </item>
    
      <item>
        <title>Support Vector Machine</title>
        <description>&lt;p&gt;Support vector machine (SVM) is often considered one of the best “out of the box” classifiers, and in this post I try to explain how we can come up with this alogrithm from scratch.&lt;/p&gt;

&lt;p&gt;I also implement the SMV for image classification with &lt;a href=&quot;http://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR-10 dataset&lt;/a&gt; by Python (numpy). &lt;a href=&quot;http://houxianxu.github.io/implementation/SVM.html&quot;&gt;This&lt;/a&gt; is for mutli-classification by using SVM loss.&lt;/p&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;1. Problem setting&lt;/h2&gt;
&lt;p&gt;Classification problem is to classify different objects into different categories. For simplicity, we just focus on &lt;strong&gt;binary classification&lt;/strong&gt; that y can take two values 1 or -1 (indicating two classes), and we firstly assume the two classes are linearly separable. After all, it is reasonable to solve problems from simple to complex.&lt;/p&gt;

&lt;h2 id=&quot;basic-idea-what-we-have-known&quot;&gt;2. Basic idea (What we have known)&lt;/h2&gt;
&lt;p&gt;If the data is linearly separable, our goal is to find the such a line &lt;script type=&quot;math/tex&quot;&gt;f(x) = w^Tx + b = 0&lt;/script&gt; (2-dimension) that divides the plane into 2 parts and each part represent one class (see following figure). If the data is represented in high dimension say N-dimension, what we need to do is to find a hyperplane &lt;script type=&quot;math/tex&quot;&gt;w^Tx + b = 0&lt;/script&gt; which is subspace with dimension (N-1)dimension. So if &lt;script type=&quot;math/tex&quot;&gt;w^Tx + b = 0&lt;/script&gt;, the label &lt;script type=&quot;math/tex&quot;&gt;y = 1&lt;/script&gt;, otherwise &lt;script type=&quot;math/tex&quot;&gt;y = -1&lt;/script&gt;. However, the problem is that in fact there exists infinite such hyperplanes if the data can be perfectly linearly separated, because a given separating hyperplane can be shifted a tiny bit up or down, or rotated without coming into contact with any of the observations (the line 1, 2 and 3 in the following figure) . Of course we can randomly choose a separating line.&lt;/p&gt;

&lt;!-- ![Scatter Plot of Two variables](http://houxianxu.github.io/images/SVM/1.png &quot;linearly separable&quot;) --&gt;
&lt;center&gt;&lt;img src=&quot;/images/SVM/1.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;h2 id=&quot;maximal-margin-classifier&quot;&gt;3. Maximal Margin Classifier&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Can we do better?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Is that possible for us to choose the even “best” line or hyperplane from the infinit possible separating hyperplanes? So the next question is how to define the “best” hyperplane. Because the final goal is trying to use the hyperplane as decision boundary to distinguish the two classes, so we can choose the hyperplane which can make the distinction more obvious. Intuitively the separating hyperplane should be farthest from the training observations, that’s to say, the distance between the nearest observation and the hyperplane should be maximized. This distance is usually called margin and the corresponding classifier is known as maximal margin classifier, and the separating hyperplane has the farthest minimum distance to the training observations. Take the above figure for example, line 3 is better than line 1 and 2.&lt;/p&gt;

&lt;p&gt;From figure below, we can see that there are 3 training points having equal distance from the maximal margin line and the two dash lines indicate the width of margin. These 3 observations are known as &lt;strong&gt;support vectors&lt;/strong&gt;. Since these points can interpreted as n-1 dimension vectors and define the maximal margin, in other words, these vectors can “support” the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well. What’s more, the maximal margin hyperplane is only depends on the support vectors, not other observation.&lt;/p&gt;

&lt;!-- ![Support Vector](http://houxianxu.github.io/images/SVM/2.png &quot;support vector&quot;) --&gt;
&lt;center&gt;&lt;img src=&quot;/images/SVM/2.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;Calculate the maximal margin&lt;/strong&gt;
In order to calculate the maximal margin, we should figure out how to calculate the geometric margin which is the distance from a point to a line or hyperplane. As following figure, the point at A representing the input &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; of some training example. Its distance to the decision boundary (a line with (w, b)) is &lt;script type=&quot;math/tex&quot;&gt;\gamma^{(i)}&lt;/script&gt;, is given by the line segment AB. And the distance &lt;script type=&quot;math/tex&quot;&gt;\gamma^{(i)}&lt;/script&gt; can be calculate in the following way:&lt;/p&gt;

&lt;!-- ![geometric margin](http://houxianxu.github.io/images/SVM/3.png &quot;geometric margin&quot;) --&gt;
&lt;center&gt;&lt;img src=&quot;/images/SVM/3.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;p&gt;vector &lt;script type=&quot;math/tex&quot;&gt;BA = x_A - x_B&lt;/script&gt;, unit vector is &lt;script type=&quot;math/tex&quot;&gt;w/\|w\|&lt;/script&gt;, so the point B is given by &lt;script type=&quot;math/tex&quot;&gt;x^{(i)} - \gamma^{(i)} w/\|w\|&lt;/script&gt;. And point B is on the decision boundary &lt;script type=&quot;math/tex&quot;&gt;w^T x + b&lt;/script&gt;, therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^T \big(x^{(i)} - \gamma^{(i)} \frac{w}{\|w\|}\big) + b = 0&lt;/script&gt;

&lt;p&gt;Then solving &lt;script type=&quot;math/tex&quot;&gt;\gamma^{(i)}&lt;/script&gt; yields:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma^{(i)} = \frac{w^T x^{(i)} + b}{\|w\|}&lt;/script&gt;

&lt;p&gt;Using bias trick to represent the two parameters &lt;strong&gt;w&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt; as one, i.e. set &lt;script type=&quot;math/tex&quot;&gt;x_0 = 1&lt;/script&gt; and add &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; to weights vector &lt;strong&gt;w&lt;/strong&gt;.
Then we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma^{(i)} = \frac{w^T x^{(i)}}{\|w\|}&lt;/script&gt;

&lt;p&gt;Therefore based on a set of m training observations &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2, ..., x_m&lt;/script&gt; and associated class labels &lt;script type=&quot;math/tex&quot;&gt;y_1, y_2, ..., y_m \in \big\{1, -1\big\}&lt;/script&gt;, the assumption that the training set is linearly separable, the maximal margin line or hyperplane is the solution to the optimization problem.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Maximize_{w, M} \:\:\: \frac{M}{\|w\|}  \:\:\:......... (1)&lt;/script&gt;

&lt;p&gt;Subject to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y^{(i)} (W^Tx^{(i)}) = M \:\: \forall i = 1, 2, ..., m \:\:\:......... (2)&lt;/script&gt;

&lt;p&gt;The constrains (2) guarantees that each observation will be on the correct side of the decision boundary and the value of &lt;script type=&quot;math/tex&quot;&gt;y^{(i)} (W^Tx^{(i)})&lt;/script&gt; is at least M, provided that M is positive. In addition, the margin is given by &lt;script type=&quot;math/tex&quot;&gt;\frac{w^T x^{(i)}}{\|w\|}&lt;/script&gt;, the objective function &lt;script type=&quot;math/tex&quot;&gt;(1) \frac{M}{\|w\|}&lt;/script&gt; ensures that each observation has at least a distance &lt;script type=&quot;math/tex&quot;&gt;\frac{M}{\|w\|}&lt;/script&gt; from the hyperplane or decision boundary. Hence, the optimization problem choose &lt;strong&gt;w&lt;/strong&gt; and &lt;strong&gt;M&lt;/strong&gt; to maximize &lt;script type=&quot;math/tex&quot;&gt;\frac{M}{\|w\|}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solve the optimization problem&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If we could solve the optimization problem above efficiently, then we would be done. In fact the optimization problem above is very difficult because we have a nasty objective &lt;script type=&quot;math/tex&quot;&gt;\frac{M}{\|w\|}&lt;/script&gt; function, which is non-convex. So can we do better?&lt;/p&gt;

&lt;p&gt;The final goal is to find the decision boundary &lt;script type=&quot;math/tex&quot;&gt;w^T x = 0&lt;/script&gt;, so multiplying w by some constant can affect the margin but doesn’t change the decision boundary. Therefore, we can set the value of &lt;script type=&quot;math/tex&quot;&gt;w^T x_0&lt;/script&gt; for the nearest point to be 1, i.e., &lt;script type=&quot;math/tex&quot;&gt;M = 1&lt;/script&gt;. Additionally maximize &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\|w\|}&lt;/script&gt; is the same to minimize |w|, again is the same thing as minimizing &lt;script type=&quot;math/tex&quot;&gt;\|w\|^2&lt;/script&gt;. Therefore we have the following optimization problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Minimize_w \:\:\: \frac{1}{2}\|w\|^2  \:\:\:......... (1)&lt;/script&gt;

&lt;p&gt;Subject to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y^{(i)} (W^Tx^{(i)}) = 1 \:\: \forall i = 1, 2, ..., m \:\:\:......... (2)&lt;/script&gt;

&lt;p&gt;The new version of optimization problem can be efficiently solved, because the objective function is a convex quadratic function and all the constrains are linear. The problem can be solved by Quadratic Program (QR) software such as &lt;a href=&quot;http://cvxopt.org&quot;&gt;CVXOPT&lt;/a&gt; for Python.&lt;/p&gt;

&lt;h2 id=&quot;dual-form-kernel-and-support-vector-machine&quot;&gt;4 Dual Form, Kernel and Support Vector Machine&lt;/h2&gt;
&lt;p&gt;According to &lt;a href=&quot;http://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf&quot;&gt;Lagrange duality&lt;/a&gt;, we can get the dual form of the above optimization problem.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Maximize_{\alpha} \:\: W(\alpha) = \sum_{(i=1)}^m \alpha_i - \frac{1}{2} \sum_{i, j=1}^m y^{(i)}y^{(j)} \alpha_i \alpha_j \langle x^{(i)}, x^{(j)}\rangle&lt;/script&gt;

&lt;p&gt;Subject to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_i \geq 0, \forall \: i = 1, 2, ..., m&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^m \alpha_i y^{(i)} = 0&lt;/script&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;\langle x^{(i)}, x^{(j)}\rangle = \big(x^{(i)}\big)^T x^{(j)}&lt;/script&gt;, and the original &lt;strong&gt;w&lt;/strong&gt; = &lt;script type=&quot;math/tex&quot;&gt;\sum_i^m \alpha_i y^{(i)}x^{(i)}&lt;/script&gt;. And the decision boundary becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = w^T + b = \big(\sum_i^m\alpha_i y^{(i)}x^{(i)}\big)^T x + b = \sum_i^m\alpha_i y^{(i)} \langle x^{(i)}, x\rangle + b = 0&lt;/script&gt;

&lt;p&gt;Therefore, we can solve the dual problem (optimizing the &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;) in lieu of solving the primal optimization problem. Specifically in order to ake a prediction, all we need to do is to calculate the inner product between the new point x and each of the training samples &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;. However, it turns out that &lt;script type=&quot;math/tex&quot;&gt;\alpha_i's&lt;/script&gt; will be zero except for the support vectors, so we only need to find the inner products between x and support vectors to make prediction.&lt;/p&gt;

&lt;p&gt;So far, what we’ve got is just a linear classifier or linear boundary &lt;script type=&quot;math/tex&quot;&gt;w^T x + b = 0&lt;/script&gt;. And if we want a non-linear boundary, what we can do? Intuitively we can use non-linear items in the boundary functions such as &lt;script type=&quot;math/tex&quot;&gt;wx^2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;wx^3&lt;/script&gt;. In general we need to use a non-linear function (g(x)) to transfer the original input x to a new value g(x) which are passed into learning algorithm. These new quantities are often called &lt;strong&gt;features&lt;/strong&gt; and the original input x can be called &lt;strong&gt;attributes&lt;/strong&gt;. Usually people use &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt; the &lt;strong&gt;feature mapping&lt;/strong&gt;, which maps from attributes to features. Here is a example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(x) =  \begin{bmatrix} x\\ x^2 \\ x^3 \end{bmatrix}&lt;/script&gt;

&lt;p&gt;Then the decision boundary is &lt;script type=&quot;math/tex&quot;&gt;f(x) = w_1 x + w_2 x^2 + w_3 x^3 + b = 0&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We should notice that the above decision boundary is a non-linear in 2-dimension space, i.e., &lt;script type=&quot;math/tex&quot;&gt;w_1 x + w_2 x^2 + w_3 y + b = 0&lt;/script&gt;, however we get a plane in a 3-dimension space &lt;script type=&quot;math/tex&quot;&gt;w_1 x + w_2 y + w_3 z + b = 0&lt;/script&gt;, which we can be solved by using maximal classifier discussed above.&lt;/p&gt;

&lt;p&gt;Thus, rather than using the original input attributes x, we may instead use the features &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt;. To do so, we just need to change the previous algorithm by replacing x with &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The next question is how to choose the feature mapping, and we could choose arbitrary non-linear functions to compute features &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt;, and then calculate the inner product of &lt;script type=&quot;math/tex&quot;&gt;\phi(x)^T \phi(z)&lt;/script&gt;. However, it may be very expensive to compute the features and the inner product when features are high dimension vectors.&lt;/p&gt;

&lt;p&gt;One important property of the dual form is that the algorithm can be written entirely in terms of inner product &lt;script type=&quot;math/tex&quot;&gt;\langle x, z\rangle&lt;/script&gt;, which means that we can replace the inner product with &lt;script type=&quot;math/tex&quot;&gt;\langle \phi(x), \phi(z) \rangle&lt;/script&gt;. And we define the &lt;strong&gt;Kernel&lt;/strong&gt; as following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x, z) = \phi(x)^T \phi(z) = \langle \phi(x), \phi(z) \rangle&lt;/script&gt;

&lt;p&gt;The goal is to compute the &lt;script type=&quot;math/tex&quot;&gt;K(x, z)&lt;/script&gt;, and the interesting is that &lt;script type=&quot;math/tex&quot;&gt;K(x, z)&lt;/script&gt; may be not expensive to calculate because we don’t firsly need to compute the &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt; and then calculate the inner product (see following example).&lt;/p&gt;

&lt;p&gt;Suppose the &lt;script type=&quot;math/tex&quot;&gt;x, z \in \mathbb{R}^n&lt;/script&gt; and we can can construct the Kernel:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x, z) = (x^T z)^2&lt;/script&gt;

&lt;p&gt;We can rewrite it as following&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
     \begin{split} 
     K(x, z)
     &amp;= (x^T z)^2 \\
     &amp;= \big(\sum_{i=1}^n x_i z_i\big) \big(\sum_{j=1}^n x_j z_j) \\
     &amp;= \sum_{i=1}^n \sum_{j=1}^n x_i x_j z_i z_j \\
     &amp;= \sum_{i, j=1}^n (x_i x_j)(z_i z_j)
    \end{split}
    \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can see &lt;script type=&quot;math/tex&quot;&gt;K(x, z) = \phi(x)^T \phi(z)&lt;/script&gt;, where the &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt; is shown below (take n = 3)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(x) = \begin{bmatrix} x_1x_1\\ x_1x_2 \\x_1x_3\\x_2x_1\\x_2x_2\\x_2x_3\\x_3x_1\\x_3x_2\\x_3x_3 \end{bmatrix}&lt;/script&gt;

&lt;p&gt;So we can efficiently calculate the &lt;script type=&quot;math/tex&quot;&gt;K(x, z) = (x^T z)^2&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt; because of n-dimension input attributes x. However, it takes &lt;script type=&quot;math/tex&quot;&gt;O(n^2)&lt;/script&gt; to calculate &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In general, we can also use &lt;script type=&quot;math/tex&quot;&gt;K(x, z) = (X^T z + c)^d&lt;/script&gt; to achieve feature mapping, which is known as &lt;strong&gt;ploynomial kernel&lt;/strong&gt; of degree &lt;strong&gt;d&lt;/strong&gt;. This kernel essentially amount to fitting a support vector classifier in a higher-dimensional space involving polynomials of degree d, which leads to a much more flexible decision boundary. Notice that though working in a very high dimension space, we only need &lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt; time to compute the K(x, z) because we never need to explicitly represent feature vectors in the very high dimensional feature space.&lt;/p&gt;

&lt;p&gt;Another popular choice is &lt;strong&gt;Gaussian Kernel&lt;/strong&gt; or &lt;strong&gt;Radial Kernel&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x, z) = exp \big( - \frac{(x-z)^2} {2 \sigma^2} \big)&lt;/script&gt;

&lt;p&gt;We can use Taylor expansion to expand the Gaussian Kernel (&lt;script type=&quot;math/tex&quot;&gt;e^x = \sum_{n=0}^\infty \frac {x^n} {n!}&lt;/script&gt;), and we can see that the feature vector that corresponds to the Gaussian kernel has infinite dimensionality, and the feature space is implicit.&lt;/p&gt;

&lt;p&gt;How does the Kernel work? One intuition is to think of &lt;script type=&quot;math/tex&quot;&gt;K(x, z)&lt;/script&gt; as a measurement of how similar are &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\phi(z)&lt;/script&gt;, or of how similar are x and z. If  &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;\phi(z)&lt;/script&gt; are close to each other, then &lt;script type=&quot;math/tex&quot;&gt;K(x, z) = \phi(x)^T \phi(z)&lt;/script&gt; is expected to large, otherwise &lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;\phi(z)&lt;/script&gt; are far apart, then &lt;script type=&quot;math/tex&quot;&gt;K(x, z)&lt;/script&gt; is small. Recall that we use the sign of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = w^T + b =\sum_i^m\alpha_i y^{(i)} \langle x^{(i)}, x\rangle + b =\sum_i^m\alpha_i y^{(i)} \langle \phi(x^{(i)}), \phi(x)\rangle + b&lt;/script&gt;

&lt;p&gt;for prediction. Look at Gaussian Kernel, if training observations that are far from test observation x will play essentially little role in the predicted class label for x. This means that Gaussian Kernel has a local hehavior, in the sense that only nearby training observations have a big effect on a class label for test observation.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Support Vector Machine&lt;/strong&gt; is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels.&lt;/p&gt;

&lt;h2 id=&quot;the-non-separable-case&quot;&gt;5 The Non-separable Case&lt;/h2&gt;
&lt;p&gt;The SVMs work very well for classification if a separating hyperplane exists, however, we will get stuck when the data is overlapped and non-separable because there is no max margin. So we can extend the separating hyperplane in order to almost separate the classes based on soft margin. We instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane. We reformulate the optimization problem as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Minimize_w \:\:\: \frac{1}{2}\|w\|^2 + C \sum_{i=1}^m \zeta_i&lt;/script&gt;

&lt;p&gt;Subject to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y^{(i)} (W^Tx^{(i)}) = 1 -\zeta_i \:\: \forall i = 1, 2, ..., m&lt;/script&gt;

&lt;p&gt;Thus, we permit the observation to be on the incorrect side of the margin, or even the incorrect side of the hyperplane (&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
1-\zeta_i &lt; 0 %]]&gt;&lt;/script&gt;), and we pay a cost of the objective function being increased by &lt;script type=&quot;math/tex&quot;&gt;C\zeta_i&lt;/script&gt;. The big number C ensuring that &lt;script type=&quot;math/tex&quot;&gt;\zeta_i&lt;/script&gt; is small and most examples have at least soft max margin.&lt;/p&gt;

&lt;p&gt;And the dual form is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Maximize_{\alpha} \:\: W(\alpha) = \sum_{(i=1)}^m \alpha_i - \frac{1}{2} \sum_{i, j=1}^m y^{(i)}y^{(j)} \alpha_i \alpha_j \langle x^{(i)}, x^{(j)}\rangle&lt;/script&gt;

&lt;p&gt;Subject to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 \leq \alpha_i \geq C, \forall \: i = 1, 2, ..., m&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^m \alpha_i y^{(i)} = 0&lt;/script&gt;

&lt;p&gt;Above is the basic idea of Support Vector Machine (SVM), all that remains is to to find a algorithm for solving the dual problem. The SMO (sequential minimal optimization) algorithm give an efficient way to solve the dual problem. You can find the details &lt;a href=&quot;http://cs229.stanford.edu/materials/smo.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;multiclass-classification&quot;&gt;6 Multiclass classification&lt;/h2&gt;
&lt;p&gt;We need to generalize to the multiple class case, that’s to say, the value of y is not binary any more, instead y can equal to 0, 1, 2, …, k.&lt;/p&gt;

&lt;p&gt;####Transfer multi-class classification into binary classification problem&lt;/p&gt;

&lt;p&gt;We need change multiple classes into two classes, and the idea is to construct several logistic classifier for each class. We set the value of y (label) of one class to 1, and 0 for other classes. Thus, if we have K classes, we build K SVM and use it for prediction. The idea is the same as use &lt;a href=&quot;http://houxianxu.github.io/logistic-softmax-regression/#multiclass&quot;&gt;logistic regression&lt;/a&gt; for multi-classfication.
&lt;!-- ![One vs all](http://houxianxu.github.io/images/logisticRegression/4.png &quot;Figure 4&quot;) --&gt;&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/images/logisticRegression/4.png&quot; width=&quot;80%&quot; /&gt;&lt;/center&gt;

&lt;h4 id=&quot;multiclass-support-vector-machine-loss&quot;&gt;Multiclass Support Vector Machine loss&lt;/h4&gt;
&lt;p&gt;Similar to &lt;a href=&quot;http://houxianxu.github.io/logistic-softmax-regression/&quot;&gt;softmax&lt;/a&gt;, For mutilple classes problems (K categoires), it is possible to establish a mapping function for each class. We can simply use a linear mapping for all classes (K mapping function):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^{(i)}, W, b) = Wx^{(i)} + b =f(x^{(i)}, W) = Wx^{(i)} \:(bias \: trick)&lt;/script&gt;

&lt;p&gt;Intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes. Thus, we can predict the test observation as the class with the highest score. Next we should find a loss function to optimize the parameters.&lt;/p&gt;

&lt;p&gt;For sample &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;, the vector &lt;script type=&quot;math/tex&quot;&gt;f(x_i, W)&lt;/script&gt; is the scores for all the classes, &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is the correct class and &lt;script type=&quot;math/tex&quot;&gt;f(x_i, W)_{y_i}&lt;/script&gt; is the score corresponding to the correct class for &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;. The score for the &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; class is &lt;script type=&quot;math/tex&quot;&gt;f(x_i, W)_j&lt;/script&gt;. The multiclass SVM loss for the &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; sample is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
     \begin{split} 
		L_i &amp;= \sum_{j\neq y_i} max(0, f(x_i, W)_j - f(x_i, W)_{y_j} + \Delta) \\
	   		&amp;= \sum_{j\neq y_i} max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)
	\end{split}
    \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Though the expression seems complex, the interpretation is relatively simple. Firstly every class contribute to the loss of one sample, and the correct class doesn’t lead to loss. We want the correct class for sample &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; have a score &lt;script type=&quot;math/tex&quot;&gt;f(x_i, W)_{y_j}&lt;/script&gt; higher than the incorrect classes &lt;script type=&quot;math/tex&quot;&gt;f(x_i, W)_j&lt;/script&gt; by some fixed margin. If the incorrect class score adds some fix margin still less than correct class score, i.e., &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
f(x_i, W)_j + \Delta &lt; f(x_i, W)_{y_j} %]]&gt;&lt;/script&gt;, then set the loss to be zero. Because the correct score is “much” big than than the incorrect scores, which we desire to achieve. However, if the the correct class score is not “big” enough or even less than the incorrect class scores, then we set the loss to be &lt;script type=&quot;math/tex&quot;&gt;f(x_i, W)_j + \Delta - f(x_i, W)_{y_j}&lt;/script&gt;. Additionally the function max(0, -) is often called the &lt;strong&gt;hinge loss&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We still need regularization to our loss function. Suppose that we’ve got a set of weights &lt;strong&gt;W&lt;/strong&gt; that can correctly classify all the samples, then the set of &lt;strong&gt;W&lt;/strong&gt; is not necessarily unique. Firstly if we multiply a number &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; &lt;strong&gt;W&lt;/strong&gt;, then the decision boundary remains the same. So the scores stretches accordingly but the magin &lt;script type=&quot;math/tex&quot;&gt;\Delta&lt;/script&gt; doesn’t change. Usually people add &lt;script type=&quot;math/tex&quot;&gt;L_2&lt;/script&gt; regularization penalty &lt;strong&gt;R(W)&lt;/strong&gt; to loss function.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(W) = \sum_k \sum_l W_{k, l}^2&lt;/script&gt;

&lt;p&gt;So the full loss is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
     \begin{split}  
		L &amp;= \frac{1}{m} \sum_i L_i + \lambda R(W) \\
		  &amp;= \frac{1}{m} \sum_i \sum_{j \neq y_{y_i}} [max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)] + \lambda \sum_k \sum_l W_{k, l}^2
	\end{split}
    \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;When &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is big, then &lt;script type=&quot;math/tex&quot;&gt;R(W) = \sum_k \sum_l W_{k, l}^2&lt;/script&gt; is small. From binary SVM above, we know that the distance between one observation and the hyperplane of correct class is &lt;script type=&quot;math/tex&quot;&gt;\frac{f(x_i, W)_{y_j}} {\|w_{k}\|}&lt;/script&gt;. therefore, the &lt;script type=&quot;math/tex&quot;&gt;L_2&lt;/script&gt; penalty leads to the &lt;strong&gt;max margin&lt;/strong&gt; property in SVMs and improve the generalization of the performance of the classifiers and avoid overfitting.&lt;/p&gt;

&lt;p&gt;This loss function has no constrains and we can calculate the gradient and optimize the &lt;strong&gt;W&lt;/strong&gt; using gradient descent algorithm.
For single example the SVM loss is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_i = \sum_{j\neq y_i} max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)&lt;/script&gt;

&lt;p&gt;We can differentiate the function with respect to weights. For &lt;strong&gt;w&lt;/strong&gt; corresponding to the correct class:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{w_{y_i}} L_i = - \big(\sum_{y \neq y_i} \mathbb{1}(w_j^T x_i - w_{y_i}^T x_i + \Delta 0)\big) x_i&lt;/script&gt;

&lt;p&gt;The gradient for incorrect class:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{w_j} L_i = \mathbb{1}(w_j^T x_i - w_{y_i}^T x_i + \Delta 0) x_i&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\mathbb{1}&lt;/script&gt; is the indicator function that is one if the condition is true or zero otherwise.&lt;/p&gt;

&lt;h2 id=&quot;get-your-hands-dirty-and-have-fun&quot;&gt;7 Get your hands dirty and have fun&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Purpose: Implement multi-classification classifier.&lt;/li&gt;
  &lt;li&gt;Data: CIFAR-10 dataset, consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The data is available &lt;a href=&quot;http://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Setup: I choose Python (IPython, numpy etc.) on Mac for implementation, and the results are published in a IPython notebook.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://houxianxu.github.io/implementation/SVM.html&quot;&gt;click here&lt;/a&gt; for the implementation.&lt;/li&gt;
  &lt;li&gt;Following is code to implement the logistic, one-vs-all and softmax classifiers by gradient decent algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;classifiers: algorithms/classifiers.py&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;algorithms.classifiers.loss_grad_logistic&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;algorithms.classifiers.loss_grad_softmax&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;algorithms.classifiers.loss_grad_svm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LinearClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# set up the weight matrix &lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sgd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Train linear classifer using batch gradient descent or stochastic gradient descent

        Parameters
        ----------
        X: (D x N) array of training data, each column is a training sample with D-dimension.
        y: (N, ) 1-dimension array of target data with length N.
        method: (string) determine whether using 'bgd' or 'sgd'.
        batch_size: (integer) number of training examples to use at each step.
        learning_rate: (float) learning rate for optimization.
        reg: (float) regularization strength for optimization.
        num_iters: (integer) number of steps to take when optimization.
        verbose: (boolean) if True, print out the progress (loss) when optimization.

        Returns
        -------
        losses_history: (list) of losses at each training iteration
        &quot;&quot;&quot;&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# assume y takes values 0...K-1 where K is number of classes&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# initialize the weights with small values&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# just need weights for one class&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# weigths for each class&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;losses_history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'bgd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;# randomly choose a min-batch of samples&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# grad =[K x D]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;losses_history&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# update weights&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [K x D]&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# print self.W&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# print 'dsfad', grad.shape&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'iteration &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d: loss &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;losses_history&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Predict value of y using trained weights

        Parameters
        ----------
        X: (D x N) array of data, each column is a sample with D-dimension.

        Returns
        -------
        pred_ys: (N, ) 1-dimension array of y for N sampels
        h_x_mat: Normalized scores
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;f_x_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__class__&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Logistic'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_x_mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# multiclassification&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_x_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# normalized score&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h_x_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_x_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, N]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h_x_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_x_mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_x_mat&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Compute the loss and gradients.

        Parameters
        ----------
        The same as self.train()

        Returns
        -------
        a tuple of two items (loss, grad)
        loss: (float)
        grad: (array) with respect to self.W
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Subclasses of linear classifier&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Logistic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;A subclass for binary classification using logistic function&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_grad_logistic_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_grad_logistic_naive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;A subclass for multi-classicication using Softmax function&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_grad_softmax_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_grad_softmax_naive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SVM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;A subclass for multi-classicication using SVM function&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_grad_svm_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Function to compute loss and gradients for SVM classification: algorithms/classifiers/loss_grad_svm.py&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# file: algorithms/classifiers/loss_grad_svm.py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad_svm_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Compute the loss and gradients using softmax function 
    with loop, which is slow.

    Parameters
    ----------
    W: (K, D) array of weights, K is the number of classes and D is the dimension of one sample.
    X: (D, N) array of training data, each column is a training sample with D-dimension.
    y: (N, ) 1-dimension array of target data with length N with lables 0,1, ... K-1, for K classes
    reg: (float) regularization strength for optimization.

    Returns
    -------
    a tuple of two items (loss, grad)
    loss: (float)
    grad: (K, D) with respect to W
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# compute all scores&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [C x N] matrix&lt;/span&gt;
 
    &lt;span class=&quot;c&quot;&gt;# get the correct class score &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;correct_class_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1 x N]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;margins_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct_class_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [C x N]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# set the negative score to be 0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;margins_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;margins_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;margins_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;margins_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# add regularization to loss&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# compute gradient&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores_mat_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores_mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# compute the number of margin 0 for each sample&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;margins_mat&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores_mat_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;margins_mat&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores_mat_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_pos&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# compute dW&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_mat_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;reference-and-further-reading&quot;&gt;11. Reference and further reading&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Andrew Ng’s &lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;Machine learning on Coursera&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Machine learing notes on &lt;a href=&quot;http://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf&quot;&gt;Stanford Engineering Everywhere (SEE)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Stanford University open course &lt;a href=&quot;http://vision.stanford.edu/teaching/cs231n/&quot;&gt;CS231n&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The University of Nottingham &lt;a href=&quot;http://modulecatalogue.nottingham.ac.uk/Nottingham/asp/moduledetails.asp?year_id=000113&amp;amp;crs_id=021211&quot;&gt;Machine Learning Module&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ## New idea:
- asymmetric prediction, max margins for two classes are different.
- Two mimimal margin classifier
 --&gt;

</description>
        <pubDate>Sat, 25 Apr 2015 00:00:00 +0800</pubDate>
        <link>http://houxianxu.github.io/2015/04/25/support-vector-machine/</link>
        <guid isPermaLink="true">http://houxianxu.github.io/2015/04/25/support-vector-machine/</guid>
        
        
      </item>
    
      <item>
        <title>Logistic and Softmax Regression</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;In this post, I try to discuss how we could come up with the logistic and softmax regression for classification. I also implement the algorithms for image classification with &lt;a href=&quot;http://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR-10 dataset&lt;/a&gt; by Python (numpy). &lt;a href=&quot;http://houxianxu.github.io/implementation/LogisticRegression.html&quot;&gt;The first one&lt;/a&gt;) is binary classification using logistic regression, &lt;a href=&quot;http://houxianxu.github.io/implementation/One-vs-All-LogisticRegression.html&quot;&gt;the second one&lt;/a&gt; is multi-classification using logistic regression with one-vs-all trick and &lt;a href=&quot;http://houxianxu.github.io/implementation/SoftmaxRegression.html&quot;&gt;the last one&lt;/a&gt;) is mutli-classification using softmax regression.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;1. Problem setting&lt;/h2&gt;
&lt;p&gt;Classification problem is to classify different objects into different categories. It is like regression problem, except that the predictor y just has a small number of discrete values. For simplicity, we just focus on &lt;strong&gt;binary classification&lt;/strong&gt; that y can take two values 1 or 0 (indicating two classes).&lt;/p&gt;

&lt;h2 id=&quot;basic-idea&quot;&gt;2. Basic idea&lt;/h2&gt;
&lt;p&gt;We could plot the data on a 2-D plane and try to figure out whether there is any structure of the data (see following figure).&lt;/p&gt;

&lt;!-- ![Scatter Plot of Two variables](http://houxianxu.github.io/images/logisticRegression/1.png &quot;Figure 1&quot;) --&gt;
&lt;p&gt;&lt;img src=&quot;/images/logisticRegression/1.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the particular example above, it is not hard to figure out we could find a line to separate the two classes. Specifically we divide the 2-D plane into 2 parts according to a line, and then we can predict new sample by observing which part it belongs to. Mathematically if &lt;script type=&quot;math/tex&quot;&gt;z = w_0 + w_1x_1 + w_2x_2&lt;/script&gt; &amp;gt;= 0, then y = 1; if &lt;script type=&quot;math/tex&quot;&gt;z = w_0 + w_1x_1 + w_2x_2&lt;/script&gt; &amp;lt; 0, then y = 0. We can regard the linear function &lt;script type=&quot;math/tex&quot;&gt;w^Tx&lt;/script&gt; as a mapping from raw sample data (&lt;script type=&quot;math/tex&quot;&gt;x_1, x_2&lt;/script&gt;) to classes scores. Intuitively we wish that the “correct” class has a score that is higher than the scores of “incorrect” classes.&lt;/p&gt;

&lt;h2 id=&quot;how-to-find-the-best-line&quot;&gt;3. How to find the best line&lt;/h2&gt;
&lt;p&gt;The hypothesis is a linear model &lt;script type=&quot;math/tex&quot;&gt;w_0 + w_1x_1 + w_2x_2 = W^TX&lt;/script&gt;, the threshold is z = 0. The score value of z depends on the distance between the point and the target line, and the absolute value of z could be very large or small. We could &lt;strong&gt;normalize the distances&lt;/strong&gt; for convenience, however, we had better not use linear normalization such as x / (max(x) - min(x)) and x / (std(x)), because the distinction between the two classes is more obvious when the absolution value of z is larger. Sigmoid or logistic function is well-known to be used here, following is the function and plot of sigmoid function.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(z) = \frac{1}{1 + e^{-z}}&lt;/script&gt;

&lt;!-- ![Sigmoid function](http://houxianxu.github.io/images/logisticRegression/2.png &quot;Figure 2&quot;) --&gt;
&lt;p&gt;&lt;img src=&quot;/images/logisticRegression/2.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The new model for classification is:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;h(x) = \frac{1}{1 + e^{-w^Tx}}&lt;/script&gt;
We can see from the figure above that when z 0, g(z) 0.5 and when the absolute vaule of v is very large the g(z) is more close to 1. By feeding the score to sigmoid function, not only the scores can be normalized from 0 to 1, which can make it much easier to find the loss function, but also the result can be interpreted from probabilistic aspect.&lt;/p&gt;

&lt;h2 id=&quot;figure-out-the-loss-function&quot;&gt;4. Figure out the loss function&lt;/h2&gt;
&lt;p&gt;we need to find a way to measure the agreement between the predicted scores and the ground truth value.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Naive idea&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We could use least square loss after normalizing the training data, the result is as following:
&lt;script type=&quot;math/tex&quot;&gt;L_0 = \frac{1}{m} \sum_{i=1}^m(h(x^{(i)}) - y^{(i)})^2 = \frac{1}{m} \sum_{i=1}^m(\frac{1}{1 + e^{-w^Tx^{(i)}}} - y^{(i)})^2&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; is a vector for all features &lt;script type=&quot;math/tex&quot;&gt;x_j^{(i)}&lt;/script&gt; (j=0,1, … , n) for single sample i, and &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; is the target value for this example. However this loss function is not a convex function because of sigmoid function used here, which will make it very difficult to find the w to opimize the loss.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Can we do better?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Because of this is a binary classification problems, we can compute the loss for the two classes respectively. When target y = 1, the loss had better be very large when &lt;script type=&quot;math/tex&quot;&gt;h(x) = \frac{1}{1 + e^{-w^Tx}}&lt;/script&gt; is close to zero, and the loss should be very small when h(x) is close to one; in the same way, when target y = 0, the loss had better be very small when h(x) is close to zero, and the loss should be very large when h(x) is close to one. In fact, we can find this kind of function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
L(h(x), y) =\begin{cases} -log(h(x)) &amp; y = 1\\ -log(1 - h(x)) &amp; y  = 0 \end{cases} =L(h(x), y) = -ylog(h(x)) - (1-y)log(1-h(x)) %]]&gt;&lt;/script&gt;

&lt;p&gt;So the total loss: &lt;script type=&quot;math/tex&quot;&gt;L(w) = - \frac{1}{m} \sum_{i = 1}^m [y^{(i)}logh(x^{(i)}) + (1 - y^{(i)}) log(1-h(x^{(i)}))]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; is a vector for all &lt;script type=&quot;math/tex&quot;&gt;x_j&lt;/script&gt; (j=0,1, … , n), and &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; is the target value for this example.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x) = \frac{1}{1 + e^{-w^Tx}}&lt;/script&gt;

&lt;p&gt;The plots of loss function are shown below, and they meet the desirable properties discribed above.
&lt;!-- ![Loss function](http://houxianxu.github.io/images/logisticRegression/3.png &quot;Figure 3&quot;) --&gt;
&lt;img src=&quot;/images/logisticRegression/3.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;find-the-best-w-to-minimize-the-loss&quot;&gt;5 Find the best w to minimize the loss&lt;/h2&gt;
&lt;p&gt;Like &lt;a href=&quot;http://houxianxu.github.io/linear-regression-post/&quot;&gt;linear regression&lt;/a&gt; we can use &lt;strong&gt;gradient descent algorithm&lt;/strong&gt; to optimize w step by step.
Compute the gradient for just one sample:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
     \begin{split} 
     \frac{\partial}{\partial w_j} L(w) 
     &amp;= -(y \frac{1}{g(w^Tx)} - (1-y)  \frac{1}{1 - g(w^Tx)})  \frac{\partial}{\partial w_j} g(w^Tx) \\
     &amp;= -(y \frac{1}{g(w^Tx)} - (1-y)  \frac{1}{1 - g(w^Tx)})  g(w^Tx)(1 - g(w^Tx)) \frac{\partial}{\partial w_j} w^Tx \\
     &amp;= -(y(1-g(w^Tx)) - (1-y)g(w^Tx))x_j \\
     &amp;= (h(x)-y)x_j                                    
    \end{split}
    \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;So the gradients are as following when considering all the samples:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial w_j} L(w) = \frac{1}{m} \sum_{i = 1}^m (h(x)-y)x_j&lt;/script&gt;

&lt;p&gt;Then we can use &lt;strong&gt;batch decent algorithm&lt;/strong&gt; or &lt;strong&gt;stochastic decent algorithm&lt;/strong&gt; to optimize &lt;strong&gt;w&lt;/strong&gt;, i.e, &lt;script type=&quot;math/tex&quot;&gt;w := w + \alpha \frac{\partial}{\partial w_j} L(w)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can see that the gradient or partial derivative is the same as gradient of linear regression except for the h(x). We can get a better understanding of this when interpreting the loss function from probabilistic aspect.&lt;/p&gt;

&lt;h2 id=&quot;probabilistic-interpretation&quot;&gt;6. Probabilistic interpretation&lt;/h2&gt;
&lt;p&gt;Let us regard the value of h(x) as the probability:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases} P(y=1|x;w) = h(x) \\ P(y = 0 | x; w) = 1 - h(x) \end{cases} =P(y|x;w) = (h(x))^y(1-h(x))^{1-y}&lt;/script&gt;

&lt;p&gt;So the likelihood is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
     \begin{split} 
     L(w) &amp;= p(y|X; w) \\ 
     &amp;= \prod_{i = 1}^m p(y^{(i)}|x^{(i)};w) \\
     &amp;= \prod_{i = 1}^m (h(x^{(i)}))^{y^{(i)}} (1-h(x^{(i)}))^{1-y^{(i)}} \\                               
    \end{split}
    \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;And the log likelihood:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
     \begin{split} 
     l(w) = log(L(w))
     &amp;= \sum_{i = 1}^m y^{(i)} log(x^{(i)}) + (1 - y^{(i)}) log(1 - h(x^{(i)}))                            
    \end{split}
    \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;This equation is the same as the the loss function when picking minus, so minimize the loss can be interpreted as maximize the likelihood of the y when given x &lt;code class=&quot;highlighter-rouge&quot;&gt;p(y|x)&lt;/code&gt;. What’s more, the value of h(x) can be interpreted as the probability of the sample to be classified to y = 1. I think this is why most people prefer sigmoid function for normalization, theoretically we can choose other functions that smoothly increase from 0 to 1.&lt;/p&gt;

&lt;p&gt;After we optimize the w, we get a line in 2-D space and the line is usually called decision boundary (h(x) = 0.5). We can also generalize to binary classification on n-D space, and the corresponding decision boundary is a (n-1) Dimension hyperplane (subspace) in n-D space.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;multiclass&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;multiclass-classification----one-vs-all&quot;&gt;7. Multiclass classification – One vs all&lt;/h2&gt;
&lt;p&gt;We need to generalize to the multiple class case, that’s to say, the value of y is not binary any more, instead y can equal to 0, 1, 2, …, k.&lt;/p&gt;

&lt;h4 id=&quot;basic-idea----transfer-multi-class-classification-into-binary-classification-problem&quot;&gt;Basic idea – Transfer multi-class classification into binary classification problem&lt;/h4&gt;
&lt;p&gt;We need change multiple classes into two classes, and the idea is to construct several logistic classifier for each class. We set the value of y (label) of one class to 1, and 0 for other classes. Thus, if we have K classes, we build K logistic classifiers and use it for prediction. There is a potential problem that one sample might be classified to several classes or non-class. The solution is to compare all the values of h(x) and classify the sample to the class with the highest value of h(x). The idea is shown in following figure (From Andrew’s notes).&lt;/p&gt;

&lt;!-- ![One vs all](http://houxianxu.github.io/images/logisticRegression/4.png &quot;Figure 4&quot;) --&gt;

&lt;p&gt;&lt;img src=&quot;/images/logisticRegression/4.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;can-we-do-better----softmax&quot;&gt;8. Can we do better? – Softmax&lt;/h2&gt;
&lt;p&gt;In logistic regression classifier, we use linear function to map raw data (a sample) into a score z, which is feeded into logistic function for normalization, and then we interprete the results from logistic function as the probability of the “correct” class (y = 1). We just need a mapping function here because of just two classes (just need to decide whether one sample belongs to one class or not).
For multiple classes problems (K categories), it is possible to establish a mapping function for each class. As above we can simply use a linear mapping for all classes (K mapping function):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^{(i)}, W, b) = Wx{(i)} + b&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; is a vector for all features &lt;script type=&quot;math/tex&quot;&gt;x_j^{(i)}&lt;/script&gt; (j=0,1, … , n) for single sample i, and &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; is a single column vector of shape &lt;script type=&quot;math/tex&quot;&gt;[D, 1]&lt;/script&gt;. &lt;strong&gt;W&lt;/strong&gt; is a matrix of shape &lt;script type=&quot;math/tex&quot;&gt;[K, D]&lt;/script&gt; called &lt;strong&gt;weights&lt;/strong&gt;, &lt;strong&gt;K&lt;/strong&gt; is the number of categories, and &lt;strong&gt;b&lt;/strong&gt; is a vector of &lt;script type=&quot;math/tex&quot;&gt;[K, 1]&lt;/script&gt; called &lt;strong&gt;bias vector&lt;/strong&gt;. It is a little cumbersome to keep track of two sets of parameters (&lt;strong&gt;W&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt;), in factor we can combine the two into a single matrix. Specifically we can extend the feature vector &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; with an addition bias dimension holding constant 1, while extending &lt;strong&gt;W&lt;/strong&gt; matrix with a new column (at the first or last column). Thus we get score mapping function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^{(i)}, W) = Wx^{(i)}&lt;/script&gt;

&lt;p&gt;Where &lt;strong&gt;W&lt;/strong&gt; is a matrix of shape &lt;script type=&quot;math/tex&quot;&gt;[K, D+1]&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; is vector of shape &lt;script type=&quot;math/tex&quot;&gt;[D+1, 1]&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;f(x^{(i)}, W)&lt;/script&gt; is a vector of shape &lt;script type=&quot;math/tex&quot;&gt;[K, 1]&lt;/script&gt; indicating the different scores of every class for the &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; sample.&lt;/p&gt;

&lt;h4 id=&quot;find-the-loss-function&quot;&gt;Find the loss function&lt;/h4&gt;
&lt;p&gt;Similar to logistic regression classifier, we need to normalize the scores from 0 to 1. However we should not use a linear normalization as discussed in the logistic regression because the bigger the score of one class is, the more chance the sample belongs to this category. What’s more, the chance is similar high when the scores are very large (see the plot of logistic function above).
Similar to logistic function, people use exponential function (non-linear) to preprocess the scores and then compute the percentage of each score in the sum of all the scores. What’s more, the percentages can be interpreted as the probability of each class for one sample. Here is formula for the &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; sample:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x^{(i)}) = \frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}&lt;/script&gt;

&lt;p&gt;Here is the plot of h(x) for two classes in 3D space, you can rotate the graph by clicking the arrows to get a better understanding the shape of the h(x).
Where &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; is vector of all features of sample i, &lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt; is the weights for the &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; class, and &lt;script type=&quot;math/tex&quot;&gt;y_j&lt;/script&gt; is the correct class for the &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; sample.&lt;/p&gt;

&lt;object classid=&quot;clsid:d27cdb6e-ae6d-11cf-96b8-444553540000&quot; codebase=&quot;http://fpdownload.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=7,0,0,0&quot; width=&quot;600&quot; height=&quot;700&quot; id=&quot;function_plotter&quot; align=&quot;middle&quot;&gt;
  &lt;param name=&quot;movie&quot; value=&quot;http://dwudljvm154gg.cloudfront.net/graph3d.swf?lpf=e^x / (e^x%2Be^y)&amp;amp;lpxmin=-5&amp;amp;lpxmax=5&amp;amp;lpymin=-5&amp;amp;lpymax=5&amp;amp;lpzmin=0&amp;amp;lpzmax=1&quot; /&gt;
  &lt;param name=&quot;quality&quot; value=&quot;high&quot; /&gt;
  &lt;param name=&quot;bgcolor&quot; value=&quot;#ffffff&quot; /&gt;
  &lt;embed src=&quot;http://dwudljvm154gg.cloudfront.net/graph3d.swf?lpf=e^x / (e^x%2Be^y)&amp;amp;lpxmin=-3&amp;amp;lpxmax=3&amp;amp;lpymin=-3&amp;amp;lpymax=3&amp;amp;lpzmin=0&amp;amp;lpzmax=1&quot; quality=&quot;high&quot; bgcolor=&quot;#ffffff&quot; width=&quot;700&quot; height=&quot;750&quot; name=&quot;function_plotter&quot; align=&quot;middle&quot; allowscriptaccess=&quot;sameDomain&quot; type=&quot;application/x-shockwave-flash&quot; pluginspage=&quot;http://www.macromedia.com/go/getflashplayer&quot; /&gt;
&lt;/object&gt;

&lt;p&gt;So why exponential function? In my opinion, it is natually to come up with.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is a very simple and widely used non-linear function&lt;/li&gt;
  &lt;li&gt;This function is strictly increasing&lt;/li&gt;
  &lt;li&gt;This function is a convex function and its derivative is strictly increasing. That’s to say, when the score is large, then make it even more larger.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The lesson is that we should put exponential function in our toolbox for non-linear problems&lt;/p&gt;

&lt;p&gt;After normalizing the scores, we can use the same concept to define the loss function, which should make the loss small when the normalized score of h(x) is large, and penlize more when h(x) is small. Thus, we can use &lt;script type=&quot;math/tex&quot;&gt;-log(h(x))&lt;/script&gt; to compute the loss, and the loss for one sample is as following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_i = -log \big(h(x^{(i)})\big) = -log \big(\frac{e^{f_{y_j}^{(i)}}} {\sum_{j = 1}^k e^{f_j^{(i)}}}\big) = -log \big(\frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}\big)&lt;/script&gt;

&lt;p&gt;Total loss for all sample is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \frac{1}{m} \sum_{i = 1}^m L_i = - \frac{1}{m} \sum_{i = 1}^m log \big(h(x^{(i)})\big) = - \frac{1}{m} \sum_{i = 1}^m log \big(\frac{e^{f_{y_j}^{(i)}}} {\sum_{j = 1}^k e^{f_j^{(i)}}}\big) = - \frac{1}{m} \sum_{i = 1}^m log \big(\frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}\big)&lt;/script&gt;

&lt;h4 id=&quot;calculate-the-gradient-one-sample&quot;&gt;Calculate the gradient (one sample)&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
  \begin{split} 
      \nabla_{w_j} L_i &amp;= - \nabla_{w_j} log \big(\frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}\big) \\
      &amp;= -\nabla_{w_j} \big(w_{y_j}^Tx^{(i)}\big) + \nabla_{w_j} log \big(\sum_{j = 1}^k e^{w_j^Tx^{(i)}}\big)  \end{split}
  \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;So the gradient with respect to &lt;script type=&quot;math/tex&quot;&gt;w_{y_j}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;y_j&lt;/script&gt; is the correct class):
&lt;script type=&quot;math/tex&quot;&gt;\nabla_{w_{y_j}} = -x^{(i)} + \frac{e^{w_j^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}} x^{(i)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The gradient with respect to &lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt;:
&lt;script type=&quot;math/tex&quot;&gt;\nabla_{w_j} = \frac{e^{w_j^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}} x^{(i)}&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;is-there-any-problem-with-the-loss-function&quot;&gt;Is there any problem with the loss function&lt;/h4&gt;
&lt;p&gt;When writing code to implement the softmax function in practice, we should first compute the intermediate terms &lt;script type=&quot;math/tex&quot;&gt;e^{f_j}&lt;/script&gt; to make the scores bigger and use a logarithm function to make the score smaller. However, the value of &lt;script type=&quot;math/tex&quot;&gt;e^{f_j}&lt;/script&gt; may be very large due to the exponentials and dividing large numbers could be numerically unstable, so we should make &lt;script type=&quot;math/tex&quot;&gt;e^{f_j}&lt;/script&gt; smaller before division. Here is the trick by multiply the numerator and denominator by a constant C:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{e^{f_{y_j}^{(i)}}} {\sum_{j = 1}^k e^{f_j^{(i)}}} = \frac{C e^{f_{y_j}^{(i)}}} {C \sum_{j = 1}^k e^{f_j^{(i)}}} = \frac{e^{f_{y_j}^{(i)} + logC}} {\sum_{j = 1}^k e^{f_j^{(i)} + logC}}&lt;/script&gt;

&lt;p&gt;Because we have the flexibility to choose any number of C, we can choose C to make &lt;script type=&quot;math/tex&quot;&gt;e^{f_j^{(i)}} + logC&lt;/script&gt; small. A common choice for C is to set &lt;script type=&quot;math/tex&quot;&gt;logC = -max_jf_j^{(i)}&lt;/script&gt;. This trick makes the highest value of &lt;script type=&quot;math/tex&quot;&gt;f_j^{(i)} + logC&lt;/script&gt; to be zero and less than 0 for others. So the values of &lt;script type=&quot;math/tex&quot;&gt;e^{f_j^{(i)}} + logC&lt;/script&gt; are restricted from 0 to 1, which should be more appropriate for division.&lt;/p&gt;

&lt;h4 id=&quot;probabilistic-interpretation-1&quot;&gt;Probabilistic interpretation&lt;/h4&gt;
&lt;p&gt;We can interpret &lt;script type=&quot;math/tex&quot;&gt;h(x) = P(y^{(i)}) = \frac{e^{w_{y_j}^Tx^{(i)}}} {\sum_{j = 1}^k e^{w_j^Tx^{(i)}}}&lt;/script&gt; as the normalized probability of assigned to the correct label &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; given sample x^{(i)} and parameters &lt;strong&gt;W&lt;/strong&gt;. Firstly the score &lt;script type=&quot;math/tex&quot;&gt;f(x^{(i)}, W) = Wx^{(i)}&lt;/script&gt; can be interpreted as the unnormalized log probabilities. Then exponentiating the scores with on-linear function &lt;script type=&quot;math/tex&quot;&gt;e^x&lt;/script&gt; gives the unnormalized probabilities (may call frequency). Last using division for normalization to make the probabilities sum to one. Like logistic regression, the minimize the negative log likelihood of the correct class can also be interpreted as performing &lt;strong&gt;Maximum Likelihood Estimation&lt;/strong&gt;. The loss function can be also deduced from probabilistic theory like logistic regression, in fact linear regression, logistic regression and softmax regression all belong to &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_linear_model&quot;&gt;Generalized Linear Model&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;regularization-to-avoid-overfitting&quot;&gt;8. Regularization to avoid overfitting&lt;/h2&gt;
&lt;p&gt;In practice we often add a &lt;strong&gt;regularization loss&lt;/strong&gt; to the loss function provided above to penalize large &lt;strong&gt;weights&lt;/strong&gt; to improve generalization. The most common regularization penalty &lt;strong&gt;R(W)&lt;/strong&gt; is the &lt;strong&gt;L2&lt;/strong&gt; norm.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(W) =  \sum_k  \sum_d W_{k, d}^2&lt;/script&gt;

&lt;p&gt;So the total loss is the &lt;strong&gt;data loss&lt;/strong&gt; and the &lt;strong&gt;regularization loss&lt;/strong&gt;, so the full loss becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \frac{1}{m} \sum_{i = 1}^m L_i + \frac{1}{2} \lambda \sum_k  \sum_d W_{k, d}^2&lt;/script&gt;

&lt;p&gt;The advantage of penalizing large weights is to improve generalization and make the trained model work well for unseen data, because it means that no input dimension can have a very large influence on the scores all by itself and the final classifier is encouraged to take into account allnput dimensions to small amounts rather than a few dimensions and very strongly. Note that biases do not have the same effect as other parameters and do not control the strength of influence of an input dimension. So some people only regularize the weights &lt;strong&gt;W&lt;/strong&gt; but not the biases, however, I regularize both in the implementation both for simplicity and better performance.&lt;/p&gt;

&lt;p&gt;I have written &lt;strong&gt;another post&lt;/strong&gt; to discuss regularization in more details, especially how to interpret it. You can find the post &lt;a href=&quot;&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;get-your-hands-dirty-and-have-fun&quot;&gt;9. Get your hands dirty and have fun&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Purpose: Implement logistic regression and softmax regression classifier.&lt;/li&gt;
  &lt;li&gt;Data: CIFAR-10 dataset, consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The data is available &lt;a href=&quot;http://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Setup: I choose Python (IPython, numpy etc.) on Mac for implementation, and the results are published in a IPython notebook.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://houxianxu.github.io/implementation/LogisticRegression.html&quot;&gt;click here&lt;/a&gt; for logistic regression classification.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://houxianxu.github.io/implementation/One-vs-All-LogisticRegression.html&quot;&gt;click here&lt;/a&gt; for logistic multi-classification by one-vs-all trick.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://houxianxu.github.io/implementation/SoftmaxRegression.html&quot;&gt;click here&lt;/a&gt; for softmax multi-classification.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Following is code to implement the logistic, one-vs-all and softmax classifiers by gradient decent algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;classifiers: algorithms/classifiers.py&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# file: algorithms/classifiers.py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;algorithms.classifiers.loss_grad_logistic&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; 

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LinearClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# set up the weight matrix &lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sgd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Train linear classifier using batch gradient descent or stochastic gradient descent

        Parameters
        ----------
        X: (D x N) array of training data, each column is a training sample with D-dimension.
        y: (N, ) 1-dimension array of target data with length N.
        method: (string) determine whether using 'bgd' or 'sgd'.
        batch_size: (integer) number of training examples to use at each step.
        learning_rate: (float) learning rate for optimization.
        reg: (float) regularization strength for optimization.
        num_iters: (integer) number of steps to take when optimization.
        verbose: (boolean) if True, print out the progress (loss) when optimization.

        Returns
        -------
        losses_history: (list) of losses at each training iteration
        &quot;&quot;&quot;&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# assume y takes values 0...K-1 where K is number of classes&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# initialize the weights with small values&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__class__&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Logistic'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# just need weights for one class&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# weigths for each class&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;losses_history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'bgd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;# randomly choose a min-batch of samples&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# grad =[K x D]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;losses_history&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# update weights&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [K x D]&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# print self.W&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# print 'dsfad', grad.shape&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'iteration &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d: loss &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;losses_history&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Predict value of y using trained weights

        Parameters
        ----------
        X: (D x N) array of data, each column is a sample with D-dimension.

        Returns
        -------
        pred_ys: (N, ) 1-dimension array of y for N sampels
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__class__&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Logistic'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# multiclassification&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Compute the loss and gradients.

        Parameters
        ----------
        The same as self.train()

        Returns
        -------
        a tuple of two items (loss, grad)
        loss: (float)
        grad: (array) with respect to self.W
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### Subclasses of linear classifier&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Logistic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;A subclass for binary classification using logistic function&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_grad_logistic_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_grad_logistic_naive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;A subclass for multi-classicication using Softmax function&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_grad_softmax_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_grad_softmax_naive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Function to compute loss and gradients for logistic classification: algorithms/classifiers/loss_grad_logistic.py&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# file: algorithms/classifiers/loss_grad_logistic.py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad_logistic_naive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Compute the loss and gradients using logistic function 
    with loop, which is slow.

    Parameters
    ----------
    W: (1, D) array of weights, D is the dimension of one sample.
    X: (D x N) array of training data, each column is a training sample with D-dimension.
    y: (N, ) 1-dimension array of target data with length N.
    reg: (float) regularization strength for optimization.

    Returns
    -------
    a tuple of two items (loss, grad)
    loss: (float)
    grad: (array) with respect to self.W
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, D]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sample_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;f_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_x&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [D, ]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# add regularization&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# add regularization&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad_logistic_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Compute the loss and gradients with weights, vectorized version&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, D]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# print W&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f_x_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, D] * [D, N]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h_x_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_x_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, N]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_x_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_x_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_x_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, D]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Function to compute loss and gradients for softmax classification: algorithms/classifiers/loss_grad_softmax.py&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# file: algorithms/classifiers/loss_grad_softmax.py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad_softmax_naive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Compute the loss and gradients using softmax function 
    with loop, which is slow.

    Parameters
    ----------
    W: (K, D) array of weights, K is the number of classes and D is the dimension of one sample.
    X: (D, N) array of training data, each column is a training sample with D-dimension.
    y: (N, ) 1-dimension array of target data with length N with lables 0,1, ... K-1, for K classes
    reg: (float) regularization strength for optimization.

    Returns
    -------
    a tuple of two items (loss, grad)
    loss: (float)
    grad: (K, D) with respect to W
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sample_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [K, 1] unnormalized score&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# Shift the scores so that the highest value is 0&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;correct_class&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sum_exp_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;corr_cls_exp_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corr_cls_exp_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum_exp_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_x&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# compute the gradient&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;percent_exp_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum_exp_scores&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;percent_exp_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_x&lt;/span&gt;


        &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_x&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# deal with the correct class&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# add regularization&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad_softmax_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Compute the loss and gradients using softmax with vectorized version&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [K, N]&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Shift scores so that the highest value is 0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores_exp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;correct_scores_exp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [N, ]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores_exp_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores_exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [N, ]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct_scores_exp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_exp_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;scores_exp_normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_exp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_exp_sum&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# deal with the correct class&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores_exp_normalized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [K, N]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_exp_normalized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;summary&quot;&gt;10. Summary&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Logitic and softmax regression are similar and used to solve binary and multiple classification problems respectively. However, we can also use the logistic regression classifier to solve multi-classification based on one-vs-all trick.&lt;/li&gt;
  &lt;li&gt;We should keep it in mind that logistic and softmax regression is based on the assumption that we can use a linear model to (roughly) distinguish different classes. So we should be very careful if we don’t known the distribution of the data.&lt;/li&gt;
  &lt;li&gt;We use linear function to map the input X (such as image) to label scores y for each class: &lt;script type=&quot;math/tex&quot;&gt;scores = f(x^{(i)}, W, b) = Wx{(i)} + b&lt;/script&gt;. And then use the largest score for prediction.&lt;/li&gt;
  &lt;li&gt;Normalizing the scores from 0 to 1. Im my opinion here is the most fundamental idea of the losgistic and softmax regression (function): that is we use a non-linear (exponential function) instead of linear function for normalization. It is reasonable to interprete that the bigger the score of one class is, the even more chance the sample belongs to that category, and the it is better to make derivative strictly increasing (exponential function is an appropriate condidate). Then we normalized the scores by computing the perentage of exponent score of each class in total exponent scores for all classes.&lt;/li&gt;
  &lt;li&gt;As for loss function, the idea is to make the loss small when the normalized score is large, and penlize more when normalized score is small. it is not hard to figure out to using &lt;script type=&quot;math/tex&quot;&gt;-log(x)&lt;/script&gt; function because we use exponential function to preprocess the scores.&lt;/li&gt;
  &lt;li&gt;After defining the loss function, we can use the gradient descent algorithm to train the model.&lt;/li&gt;
  &lt;li&gt;For implementation, it is critical to use matrix calculation, however it is not straightforward to transfer the naive loop version to vectorized version, which requires a very deep understanding of matrix multiplication. I’ve implemented the two algorithms to solve the CIFAR-10 dataset, and for test datasets I’ve got 82.95% accuracy for binary classification, 33.46% for all 10-classification using one-vs-all concept and 38.32% for all 10-classification using Softmax regression.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference-and-further-reading&quot;&gt;11. Reference and further reading&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Andrew Ng’s &lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;Machine learning on Coursera&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Machine learing notes on &lt;a href=&quot;http://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf&quot;&gt;Stanford Engineering Everywhere (SEE)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Stanford University open course &lt;a href=&quot;http://vision.stanford.edu/teaching/cs231n/&quot;&gt;CS231n&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The University of Nottingham &lt;a href=&quot;http://modulecatalogue.nottingham.ac.uk/Nottingham/asp/moduledetails.asp?year_id=000113&amp;amp;crs_id=021211&quot;&gt;Machine Learning Module&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 23 Apr 2015 00:00:00 +0800</pubDate>
        <link>http://houxianxu.github.io/2015/04/23/logistic-softmax-regression/</link>
        <guid isPermaLink="true">http://houxianxu.github.io/2015/04/23/logistic-softmax-regression/</guid>
        
        
      </item>
    
      <item>
        <title>Linear Regression</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;This post dicusses how to come up with linear regression algorithm, specifically how to define the loss function and minimize the loss with gradient decent algorithm. I also implement the linear regression using Python (numpy) to do experiment with a datasets, and the result can be found in this &lt;a href=&quot;http://houxianxu.github.io/implementation/LinearRegression.html&quot;&gt;IPython notebook&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;1. Problem setting&lt;/h2&gt;
&lt;p&gt;We want to use a &lt;strong&gt;predictor variable&lt;/strong&gt; X to predict a &lt;strong&gt;quantitative response&lt;/strong&gt; Y, such as using living area (X) to predict the price (Y) of house.&lt;/p&gt;

&lt;h2 id=&quot;basic-idea&quot;&gt;2. Basic idea&lt;/h2&gt;
&lt;p&gt;Becuase of just two variables, we can simply visualize the data on a scatter plot, then we can predict Y by the structure of the plot (see following figure)&lt;/p&gt;

&lt;p&gt;After getting the scatter plot, we can estimate the position of potiential point when given the x value and get the corresponding value y.
&lt;img src=&quot;http://houxianxu.github.io/images/linearRegression/1.jpg&quot; alt=&quot;Scatter Plot of Two variables&quot; title=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;can-we-do-better&quot;&gt;3. Can we do better&lt;/h2&gt;
&lt;p&gt;So far it seems that the problem can be solved, however, we shold always ask the quesion, i.e., “Can we do better?”&lt;/p&gt;

&lt;p&gt;What’s the shortcoming of the above solution?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We have to firstly get the scatter plot, which is a problem when it scales to high dimension prediction problems.&lt;/li&gt;
  &lt;li&gt;We need our human’s eyes to find the position of the position of potential point. We humans are not happy with that, instead we want the computer to do all the work. In addition, we can easily get overwhelmed when amounts of prediction needed.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;better-idea&quot;&gt;4. Better idea&lt;/h2&gt;
&lt;p&gt;If we look at the structure of the scatter plot above, it is not hard to figure that the Y value is increasing when X gets bigger. So it is possible to find a model to fit all the data, then use the model instead for prediction.&lt;/p&gt;

&lt;p&gt;Then what’s kind of model we should use? Linear model may be a good choice because of its simplicity and ability to show the general trend.&lt;/p&gt;

&lt;p&gt;The next task is how to find the “best” line, such as \(y \) \(\approx\) \(f(x, w, b) =  w x + b, \) where w and b are the parameters of the function, in which w is called the &lt;strong&gt;weight&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt; is called bias, which doesn’t interact with the actual data \(x_i\). In order to find the “best” line, we need to define “best” and measure it. Ideally we hope &lt;script type=&quot;math/tex&quot;&gt;y_i == f_i&lt;/script&gt; for every sample i, so we can use the difference or loss (&lt;script type=&quot;math/tex&quot;&gt;|y_i - f_i|&lt;/script&gt;, also called &lt;strong&gt;L1 distance&lt;/strong&gt;) between the target &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; and predicted &lt;script type=&quot;math/tex&quot;&gt;f_i&lt;/script&gt; for measurement for a single sample. When considering all the samples, we want to minimize the average loss \(\frac{1}{m} \sum_{i=1}^m|f_i - y_i|\) for all samples (m is the number of samples). Alternatively &lt;strong&gt;L2 distance&lt;/strong&gt; can be used as well and the average loss is 
&lt;script type=&quot;math/tex&quot;&gt;L_2 = \frac{1}{m} \sum_i^m(f_i - y_i)^2,&lt;/script&gt; of course other measurement could be used as well.&lt;/p&gt;

&lt;p&gt;Next we need to find the w and b to minimize &lt;strong&gt;L_2(w, b)&lt;/strong&gt;, i.e.  least-squares loss, which is an optimization problem. Because of quadratic formula, we can guess &lt;script type=&quot;math/tex&quot;&gt;L_2&lt;/script&gt; is has bowl-shaped appearance in 3-dimension that &lt;script type=&quot;math/tex&quot;&gt;L_2&lt;/script&gt; in fact is a &lt;a href=&quot;http://stanford.edu/%7Eboyd/cvxbook/&quot;&gt;convex function&lt;/a&gt;. So based on college caculus，we can compute the partial derivative of w and b, then set them to be zero and compute the w and b (the bottom of the shape).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}\frac{\partial L_2}{\partial w} = \frac{\partial f(x, w, b)}{\partial w} = 0 \\ \frac{\partial L_2}{\partial b} = \frac{\partial f(x, w, b)}{\partial w} = 0 \end{cases}&lt;/script&gt;

&lt;p&gt;Above approach directly compute the best w and b based on the property of convex function, and we could ask ourselves is there other ways (say indirectly) to get w and b? Maybe we could firstly inilize w and b randomly, and then try to make it better little by little. By analogy, a blind hiker tries his best to reach the bottom of a hill, specifically try to take a step at every point.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The first approach (Random Local Search) could be try to extend on foot in a random direction and take a step only if it leads down hill.&lt;/li&gt;
  &lt;li&gt;Another better way is to follow the direction of steepest decend, which is the &lt;strong&gt;gradient&lt;/strong&gt; or &lt;strong&gt;derivative&lt;/strong&gt; of loss function at one point, and the w and b can be updated by following the best direction (gradient) and a given step (known as learning rate). Obviously the learning rate will have big impact on our algorithm; we can only get a very small progress if learing rate is too small, however when making a bigger step, we may get a higher loss because the point maybe jump to the other side of the bowl-shaped line. So we could do some research here, e.g. how to decide the learning rate (try different values with validation method), maybe we could make it dynamically. Another potential problem here is that we use all the samples to complish just one update when taking compulation complex into account. One solution is to update the parameters according to the gradient of the error with respect to one single training example only. This alogrithm is called &lt;strong&gt;stochastic gradient descent&lt;/strong&gt; or &lt;strong&gt;online gradient decent&lt;/strong&gt;, and &lt;strong&gt;batch gradient descent&lt;/strong&gt; for previous one. SGD often gets “close” to the minimum much faster than BGD, however it may never “converge” to the minimum. Another bonus is that it is possible to ensure that the parameters will converge to the global minimum rather then merely oscillate around the minimum.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;genralization-for-high-dimension-data&quot;&gt;5. Genralization for high dimension data&lt;/h2&gt;
&lt;p&gt;When there are more than 1 predictor variable, we just need to change the model as &lt;script type=&quot;math/tex&quot;&gt;y \approx f(x, w, b) = \sum_{j=1}^n x_j w_j + b = w^T x + b&lt;/script&gt; (w and x are vector, and n is the number of features), in fact we can make the expression more compact by setting b = &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_0 = 1&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;f(x, w) = w^T x.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The loss &lt;script type=&quot;math/tex&quot;&gt;L = \frac{1}{2m} \sum_{i=1}^m(f(x^{(i)}, w) - y^{(i)})^2&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; is a vector for all features &lt;script type=&quot;math/tex&quot;&gt;x_j^{(i)}&lt;/script&gt; (j=0,1, … , n) for single sample i, and &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; is the target value for this example.&lt;/p&gt;

&lt;p&gt;Compute the gradient for all w:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Analytic gradient, using calculus to compute the gradient directly&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
	 \begin{split} 
	 \frac{\partial}{\partial w_j} L(w, x) &amp;= \frac{\partial}{\partial w_j}  \frac{1}{2m} \sum_{i=1}^m  (f(x^{(i)}, w) 											  - y^{(i)})^2 \\ 
	 									   &amp;= \frac{1}{2m} \frac{\partial}{\partial w_j} \sum_{i=1}^m [(w_0 x_0^{(i)} + 	w_1 x_1^{(i)} + ... + w_n x_n^{(i)}) - y^{(i)}]^2 \\
	 									   &amp;= \frac{1}{m} \sum_{i=1}^m  (f(x^{(i)}, w) 											  - y^{(i)}) x_j^{(i)}
	\end{split}
	\end{equation} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Numerical gradient, which is an approximation approach based on the definition of derivatives (or gradient). The derivative of a 1-D function is the limit of the function with respect its input. When the function takes more than one parameters, the derivatives are called partial derivatives.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df(x)}{dx} = \lim_{h \rightarrow 0}  \frac{f(x + h) - f(x)}{h} = \lim_{h \rightarrow 0}  \frac{f(x + h) - f(x - h)}{2h}&lt;/script&gt;

&lt;p&gt;Update w by gradient decient: &lt;script type=&quot;math/tex&quot;&gt;w_j := w_j - \alpha \frac{\partial}{\partial w_j} L(w, x)&lt;/script&gt;, this regression is also called &lt;strong&gt;LMS&lt;/strong&gt; standing for “least mean squares”.&lt;/p&gt;

&lt;p&gt;Compared with two dimension model that is a line in 2-D space, we can look on n-dimension model as a n-hyperplane (subspace) in (n+1)-D space, e.g. a plane in a 3-D space.&lt;/p&gt;

&lt;h2 id=&quot;probabilistic-interpretation&quot;&gt;6. Probabilistic interpretation&lt;/h2&gt;
&lt;p&gt;After understanding LMS regression above, we should again ask “Can we do better?”. I think the key or foundation of above is the least-square loss function &lt;script type=&quot;math/tex&quot;&gt;L_2 = \frac{1}{m} \sum_i^m(f_i - y_i)^2&lt;/script&gt;. So why this is a reasonable choice?&lt;/p&gt;

&lt;p&gt;Because we just use f(w, x) to &lt;strong&gt;estimate&lt;/strong&gt; the target y and &lt;strong&gt;expectation&lt;/strong&gt; is often used for estimation, so we can interpret &lt;script type=&quot;math/tex&quot;&gt;f(w, x^{(i)}) = w^T x^{(i)}&lt;/script&gt; as the expectation of estimation. So we could add an error term &lt;script type=&quot;math/tex&quot;&gt;\epsilon^{(i)}&lt;/script&gt; to previous experession, as a result &lt;script type=&quot;math/tex&quot;&gt;y = w^T x^{(i)} + \epsilon^{(i)}&lt;/script&gt;. Because the expectation could be higher or less than the target value, we could even assume all &lt;script type=&quot;math/tex&quot;&gt;\epsilon^{(i)}&lt;/script&gt; are distributed IID (independently and identically distributed) according to a Gaussian Distribution (also called a Normal distribution) with mean zero and some variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;, i.e., &lt;script type=&quot;math/tex&quot;&gt;\epsilon^{(i)} \sim \mathcal{N} (0,\sigma^2)&lt;/script&gt;, so &lt;script type=&quot;math/tex&quot;&gt;y^{(i)} \sim \mathcal{N} (w^T x^{(i)},\sigma^2)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So far when given a vector &lt;strong&gt;w&lt;/strong&gt; and all &lt;strong&gt;&lt;script type=&quot;math/tex&quot;&gt;x_j&lt;/script&gt;&lt;/strong&gt;, we can compute the probability of &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; from the Gaussian Distribution. Naturally we want the maximize all the probability of &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; at the same time, and this method is called maximum likelihood. The corresponding likelihood function is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w; x) =  \prod_{i = i}^m \frac{1}{\sqrt{2 \, \pi} \sigma} exp({-\frac{(y^{(i)} - w^T x^{(i)})^2}{2 \sigma^2}})&lt;/script&gt;

&lt;p&gt;Instead of maximizing &lt;script type=&quot;math/tex&quot;&gt;L(w; x)&lt;/script&gt;, we can also maximize any strictly increasing function of &lt;script type=&quot;math/tex&quot;&gt;L(w; x)&lt;/script&gt;, naturally we can instead maximize likelihood &lt;script type=&quot;math/tex&quot;&gt;l(w; x)&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
	\begin{split} 
	l(w; x) &amp;= L(w; x) \\
				&amp;= \log \prod_{i = i}^m \frac{1}{\sqrt{2 \, \pi} \sigma} exp({-\frac{(y^{(i)} - w^T x^{(i)})^2}{2 \sigma^2}}) \\
				&amp;= \sum_{i=1}^m \log \frac{1}{\sqrt{2 \, \pi} \sigma} exp({-\frac{(y^{(i)} - w^T x^{(i)})^2}{2 \sigma^2}}) \\
				&amp;= m \log \frac{1}{\sqrt{2 \, \pi} \sigma} - \frac{1}{\sigma^2} \frac{1}{2} \sum_{i=1}^m (y^{(i)} - w^T x^{(i)})^2
\end{split}
	\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Because w are the only unknown parameters (assume &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; is known), we need only to minimize the second item &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\sigma^2} \frac{1}{2} \sum_{i=1}^m (y^{(i)} - w^T x^{(i)})^2&lt;/script&gt;, which could be viewed as less-square loss.&lt;/p&gt;

&lt;p&gt;When we see &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; as the unknown parameter, we could also calculate the “best” &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; to maximize the likelihood. I think the assumption that all the point have the same &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; is too strong to some degree, so if they are not the same and depend on X, we can get a different loss function.&lt;/p&gt;

&lt;h2 id=&quot;get-your-hands-dirty-and-have-fun&quot;&gt;7. Get your hands dirty and have fun&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Data: I use the data from linear regression exercise from Andrew Ng’s &lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;Machine learning on Coursera&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Setup: I choose Python (IPython, numpy etc.) on Mac for implementation, and the results are published in a IPython notebook, &lt;a href=&quot;http://houxianxu.github.io/implementation/LinearRegression.html&quot;&gt;click here &lt;/a&gt; for the details&lt;/li&gt;
  &lt;li&gt;Following is code to implement the batch and stochastic gradient decent algorithms.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# set the weight vector&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bgd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Train linear regression using batch gradient descent or stochastic gradient descent

        Parameters
        ----------
        X: (D x N) array of training data, each column is a training sample with D-dimension.
        y: (N, ) 1-dimension array of target data with length N. 
        method: (string) determine wheter use 'bgd' or 'sgd'
        learning_rate: (float) learning rate for optimization
        num_iters: (integer) number of steps to take when optimization
        verbose: (boolean) if True, print out the progress when optimization

        Returns
        -------
        losses_history: (list) of losses at each training iteration
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# initilize weights with small values&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, D]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;losses_history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'sgd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;# randomly choose a sample&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;losses_history&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Update weights using matrix computing (vectorized)&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;

            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'iteration &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d / &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d : loss &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;losses_history&lt;/span&gt;


    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Predict value of y using trained weights

        Parameters
        ----------
        X: (D x N) array of data, each column is a sample with D-dimension.

        Returns
        -------
        pred_ys: (N, ) 1-dimension array of y for N sampels
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_ys&lt;/span&gt;


    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Compute the loss and gradients

        Parameters
        ----------
        The same as self.train function

        Returns
        -------
        a tuple of two items (loss, grad)
        loss: (float)
        grad: (array) with respect to self.W 
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_loss_grad_naive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear_loss_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Compute the loss and gradients with weights, vectorized version

    Parameters and Returns are the same as LinearRegression.loss_grad, except including W as parameter
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# vectorized implementation &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, D] * [D, N]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, N] - [1, N]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, N] * [N, 1] &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, N] * [N, D]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear_loss_grad_naive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Compute the loss and gradients with weights, for loop version
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# [1, D]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sample_X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# a vector&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;8. Summary&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;We all should keep it in mind that linear regression is based on the assumption that the true model is linear or close to linear, so we should be very careful if we don’t known the true model in advance.&lt;/li&gt;
  &lt;li&gt;Most people use least square error to indicate the loss of the linear model and it can be interpretated from  probabilistic aspect, i.e., assuming that the errors are distributed IID according to a Gaussian Distribution, the probability of y based on x (&lt;code class=&quot;highlighter-rouge&quot;&gt;p(y|x)&lt;/code&gt;) for all the samples can be maximized to minimize the least square error. Of course, we can choose other loss function as long as it makes sense to measure the agreement between the predicted scores and the ground truth value.&lt;/li&gt;
  &lt;li&gt;We can use normal equation &lt;script type=&quot;math/tex&quot;&gt;W = (X^T X)^{-1} X^T y&lt;/script&gt; to compute W directly based on calculus, however it works slow when n is large, instead, gradient decent algorithm is more practical based on the bowl-shape of loss function. The basic idea is to reduce the loss step by step.&lt;/li&gt;
  &lt;li&gt;For implementation, it is critical to use matrix calculation. Not only can it speed up the computation, but also can make code simpler and conciser when compared to naive loop version.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference-and-further-reading&quot;&gt;9. Reference and further reading&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Andrew Ng’s &lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;Machine learning on Coursera&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Andrew Ng’s machine learing notes on &lt;a href=&quot;http://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf&quot;&gt;Stanford Engineering Everywhere (SEE)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://modulecatalogue.nottingham.ac.uk/Nottingham/asp/moduledetails.asp?year_id=000113&amp;amp;crs_id=021211&quot;&gt;Machine Learning Module&lt;/a&gt; in the University of Nottingham&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 31 Mar 2015 00:00:00 +0800</pubDate>
        <link>http://houxianxu.github.io/2015/03/31/linear-regression-post/</link>
        <guid isPermaLink="true">http://houxianxu.github.io/2015/03/31/linear-regression-post/</guid>
        
        
      </item>
    
      <item>
        <title>Why and How I Write This Blog</title>
        <description>&lt;h2 id=&quot;purpose-of-this-blog&quot;&gt;Purpose of this blog&lt;/h2&gt;
&lt;p&gt;The main reason that I want to set up this blog is to help me study machine learning and do research in this area. I believe that it is a very good habit to rethink and rewrite my own interpretation of what I have learned.&lt;/p&gt;

&lt;p&gt;Because of non-cs background, I think it is also an wonderful opportunity to learn something new in CS such as front-end technologies.&lt;/p&gt;

&lt;h2 id=&quot;logic-behind-every-post&quot;&gt;Logic behind every post&lt;/h2&gt;
&lt;p&gt;Most posts will discuss machine algorithms, of course, there are many good books and tutorials introducing these. However, what I want to do is not just to introduce the alogrithms, instead I assume that one algorithm had not been invented by predecessors, would we be able to come up with the algorithm?&lt;/p&gt;

&lt;p&gt;I think this kind of assumption could be very helpful to study and do research. The emphasis is about how the algorithm was invented (how to think), not just the algorithm itself. It is like “supervised learning” because the result of algorithm in fact is known to us, so we can use the result to supervise us to learn one algorithm and we can always ask ourselves why I can’t come up with this amazing solution.&lt;/p&gt;

</description>
        <pubDate>Tue, 17 Mar 2015 04:00:00 +0800</pubDate>
        <link>http://houxianxu.github.io/2015/03/17/first-blog/</link>
        <guid isPermaLink="true">http://houxianxu.github.io/2015/03/17/first-blog/</guid>
        
        
      </item>
    
  </channel>
</rss>
