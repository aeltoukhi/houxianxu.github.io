{
 "metadata": {
  "name": "",
  "signature": "sha256:1ec9ff108b249ee082f895ee39c765fe7aa4482c5646f716102a0313a1acb9ea"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Softmax regression \n",
      "\n",
      "###Building a softmax regression classifier to solve multi-classification [CIFAR-10 dataset](http://www.cs.toronto.edu/~kriz/cifar.html)\n",
      "\n",
      "- implement a fully-vectorized **loss function** for the Softmax Regression\n",
      "- implement the fully-vectorized expression for its **analytic gradient**\n",
      "- **check implementation** using **numerical gradient**\n",
      "- use a validation set to **tune the learning rate ** and **regularization strength**\n",
      "- **optimize** the loss function with **Batch Gradient Descent** and **Stochastic Gradient Descent**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Setup code for this notebook\n",
      "import random \n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# This is a bit of magic gto make matplotlib figures appear inline\n",
      "# in the notebook rather than in a new window\n",
      "%matplotlib inline\n",
      "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
      "plt.rcParams['image.interpolation'] = 'nearest'\n",
      "plt.rcParams['image.cmap'] = 'gray'\n",
      "# Some more magic so that the notebook will reload external python modules;\n",
      "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Download CIFAR-10 Data\n",
      "I use the loading function from [course code](http://cs231n.github.io/assignment1/) from Stanford University\n",
      "\n",
      "Run get_datasets.sh in terminal to download the datasets, or download from [Alex Krizhevsky](http://www.cs.toronto.edu/~kriz/cifar.html).\n",
      "\n",
      ">get_datasets.sh\n",
      ">``` bash\n",
      "># Get CIFAR10\n",
      ">wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      ">tar -xzvf cifar-10-python.tar.gz\n",
      ">rm cifar-10-python.tar.gz \n",
      "```\n",
      "\n",
      "The results of the downloading is showed in following figure.\n",
      "\n",
      "<img style=\"float: left\" src=\"images/cifar-10.png\">\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Write function to load data in data_utils.py\n",
      "# Write function to load the cifar-10 data\n",
      "# The original code is from http://cs231n.github.io/assignment1/\n",
      "# The function is in data_utils.py file for reusing.\n",
      "import cPickle as pickle\n",
      "import numpy as np\n",
      "import os\n",
      "\n",
      "def load_CIFAR_batch(filename):\n",
      "  \"\"\" load single batch of cifar \"\"\"\n",
      "  with open(filename, 'r') as f:\n",
      "    datadict = pickle.load(f)\n",
      "    X = datadict['data']\n",
      "    Y = datadict['labels']\n",
      "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
      "    Y = np.array(Y)\n",
      "    return X, Y\n",
      "\n",
      "def load_CIFAR10(ROOT):\n",
      "  \"\"\" load all of cifar \"\"\"\n",
      "  xs = []\n",
      "  ys = []\n",
      "  for b in range(1,6):\n",
      "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
      "    X, Y = load_CIFAR_batch(f)\n",
      "    xs.append(X)\n",
      "    ys.append(Y)    \n",
      "  Xtr = np.concatenate(xs)\n",
      "  Ytr = np.concatenate(ys)\n",
      "  del X, Y\n",
      "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
      "  return Xtr, Ytr, Xte, Yte"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load data and visualize samples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from algorithms.data_utils import load_CIFAR10\n",
      "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "\n",
      "def get_CIFAR10_data(num_training=49000, num_val=1000, num_test=10000, show_sample=True):\n",
      "    \"\"\"\n",
      "    Load the CIFAR-10 dataset, and divide the sample into training set, validation set and test set\n",
      "    \"\"\"\n",
      "\n",
      "    cifar10_dir = 'datasets/datasets-cifar-10/cifar-10-batches-py/'\n",
      "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
      "        \n",
      "    # subsample the data for validation set\n",
      "    mask = xrange(num_training, num_training + num_val)\n",
      "    X_val = X_train[mask]\n",
      "    y_val = y_train[mask]\n",
      "    mask = xrange(num_training)\n",
      "    X_train = X_train[mask]\n",
      "    y_train = y_train[mask]\n",
      "    mask = xrange(num_test)\n",
      "    X_test = X_test[mask]\n",
      "    y_test = y_test[mask]\n",
      "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
      "\n",
      "def visualize_sample(X_train, y_train, classes, samples_per_class=7):\n",
      "    \"\"\"visualize some samples in the training datasets \"\"\"\n",
      "    num_classes = len(classes)\n",
      "    for y, cls in enumerate(classes):\n",
      "        idxs = np.flatnonzero(y_train == y) # get all the indexes of cls\n",
      "        idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
      "        for i, idx in enumerate(idxs): # plot the image one by one\n",
      "            plt_idx = i * num_classes + y + 1 # i*num_classes and y+1 determine the row and column respectively\n",
      "            plt.subplot(samples_per_class, num_classes, plt_idx)\n",
      "            plt.imshow(X_train[idx].astype('uint8'))\n",
      "            plt.axis('off')\n",
      "            if i == 0:\n",
      "                plt.title(cls)\n",
      "    plt.show()\n",
      "    \n",
      "def preprocessing_CIFAR10_data(X_train, y_train, X_val, y_val, X_test, y_test):\n",
      "    \n",
      "    # Preprocessing: reshape the image data into rows\n",
      "    X_train = np.reshape(X_train, (X_train.shape[0], -1)) # [49000, 3072]\n",
      "    X_val = np.reshape(X_val, (X_val.shape[0], -1)) # [1000, 3072]\n",
      "    X_test = np.reshape(X_test, (X_test.shape[0], -1)) # [10000, 3072]\n",
      "    \n",
      "    # Normalize the data: subtract the mean image\n",
      "    mean_image = np.mean(X_train, axis = 0)\n",
      "    X_train -= mean_image\n",
      "    X_val -= mean_image\n",
      "    X_test -= mean_image\n",
      "    \n",
      "    # Add bias dimension and transform into columns\n",
      "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]).T\n",
      "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]).T\n",
      "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]).T\n",
      "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
      "\n",
      "\n",
      "# Invoke the above functions to get our data\n",
      "X_train_raw, y_train_raw, X_val_raw, y_val_raw, X_test_raw, y_test_raw = get_CIFAR10_data()\n",
      "visualize_sample(X_train_raw, y_train_raw, classes)\n",
      "X_train, y_train, X_val, y_val, X_test, y_test = preprocessing_CIFAR10_data(X_train_raw, y_train_raw, X_val_raw, y_val_raw, X_test_raw, y_test_raw)\n",
      "\n",
      "# As a sanity check, we print out th size of the training and test data dimenstion\n",
      "print 'Train data shape: ', X_train.shape\n",
      "print 'Train labels shape: ', y_train.shape\n",
      "print 'Validation data shape: ', X_val.shape\n",
      "print 'Validation labels shape: ', y_val.shape\n",
      "print 'Test data shape: ', X_test.shape\n",
      "print 'Test labels shape: ', y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "unsupported iterator index",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-a89f5dfa066a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Invoke the above functions to get our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mX_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mvisualize_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_CIFAR10_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-3-a89f5dfa066a>\u001b[0m in \u001b[0;36mget_CIFAR10_data\u001b[0;34m(num_training, num_val, num_test, show_sample)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: unsupported iterator index"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Softmax Regression Classifier\n",
      "The code is running in the backend, you can find it [here](http://houxianxu.github.io/logistic-softmax-regression/), or [github](https://github.com/houxianxu/houxianxu.github.io/tree/master/implementation)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test the loss and gradient and compare between two implementations\n",
      "from algorithms.classifiers import loss_grad_softmax_naive, loss_grad_softmax_vectorized\n",
      "import time\n",
      "\n",
      "# generate a rand weights W \n",
      "W = np.random.randn(10, X_train.shape[0]) * 0.001\n",
      "tic = time.time()\n",
      "loss_naive, grad_naive = loss_grad_softmax_naive(W, X_train, y_train, 0.0001)\n",
      "toc = time.time()\n",
      "print 'Naive loss: %f, and gradient: computed in %fs' % (loss_naive, toc - tic)\n",
      "\n",
      "tic = time.time()\n",
      "loss_vec, grad_vect = loss_grad_softmax_vectorized(W, X_train, y_train, 0.0001)\n",
      "toc = time.time()\n",
      "print 'Vectorized loss: %f, and gradient: computed in %fs' % (loss_vec, toc - tic)\n",
      "\n",
      "# Compare the gradient, because the gradient is a vector, we canuse the Frobenius norm to compare them\n",
      "# the Frobenius norm of two matrices is the square root of the squared sum of differences of all elements\n",
      "diff = np.linalg.norm(grad_naive - grad_vect, ord='fro')\n",
      "# Randomly choose some gradient to check\n",
      "idxs = np.random.choice(X_train.shape[0], 10, replace=False)\n",
      "print idxs\n",
      "print grad_naive[0, idxs]\n",
      "print grad_vect[0, idxs]\n",
      "print 'Gradient difference between naive and vectorized version is: %f' % diff\n",
      "del loss_naive, loss_vec, grad_naive"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Naive loss: 5.101850, and gradient: computed in 5.970861s\n",
        "Vectorized loss: 5.101850, and gradient: computed in 0.578520s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[  58 1180 1066  114 3041  684 2871  732 2226 2493]\n",
        "[ 0.96299059 -0.29548397  1.40331858  1.55453912  2.60552279  1.02290388\n",
        "  3.48212671  0.08947331  2.56720662  3.1420192 ]\n",
        "[ 0.96299059 -0.29548397  1.40331858  1.55453912  2.60552279  1.02290388\n",
        "  3.48212671  0.08947331  2.56720662  3.1420192 ]\n",
        "Gradient difference between naive and vectorized version is: 0.000000\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Write function to compute gradient numerically to test the analytic gradient\n",
      "And put the function in algorithms/gradient_check.py file for future use"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# file: algorithms/gradient_check.py\n",
      "def grad_check_sparse(f, x, analytic_grad, num_checks):\n",
      "  \"\"\"\n",
      "  sample a few random elements and only return numerical\n",
      "  in this dimensions.\n",
      "  \"\"\"\n",
      "  h = 1e-5\n",
      "\n",
      "  print x.shape\n",
      "\n",
      "  for i in xrange(num_checks):\n",
      "    ix = tuple([randrange(m) for m in x.shape])\n",
      "    print ix\n",
      "    x[ix] += h # increment by h\n",
      "    fxph = f(x) # evaluate f(x + h)\n",
      "    x[ix] -= 2 * h # increment by h\n",
      "    fxmh = f(x) # evaluate f(x - h)\n",
      "    x[ix] += h # reset\n",
      "\n",
      "    grad_numerical = (fxph - fxmh) / (2 * h)\n",
      "    grad_analytic = analytic_grad[ix]\n",
      "    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
      "    print 'numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Check gradient using numerical gradient along several randomly chosen dimenstion\n",
      "from algorithms.gradient_check import grad_check_sparse\n",
      "f = lambda w: loss_grad_softmax_vectorized(w, X_train, y_train, 0)[0]\n",
      "grad_numerical = grad_check_sparse(f, W, grad_vect, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(10, 3073)\n",
        "(7, 375)\n",
        "numerical: -3.916883 analytic: -3.916883, relative error: 2.703636e-08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(8, 2263)\n",
        "numerical: -2.292656 analytic: -2.292656, relative error: 1.424316e-08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(3, 1720)\n",
        "numerical: 0.717328 analytic: 0.717328, relative error: 3.519292e-08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(5, 2354)\n",
        "numerical: -3.118907 analytic: -3.118907, relative error: 1.349971e-08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 382)\n",
        "numerical: 0.168714 analytic: 0.168714, relative error: 1.563022e-07"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(7, 2426)\n",
        "numerical: -0.016456 analytic: -0.016456, relative error: 1.111633e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(7, 406)\n",
        "numerical: -4.598401 analytic: -4.598401, relative error: 1.310813e-08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 1215)\n",
        "numerical: 0.541616 analytic: 0.541616, relative error: 1.827953e-07"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 543)\n",
        "numerical: 0.990534 analytic: 0.990534, relative error: 6.933231e-09"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 3054)\n",
        "numerical: -0.067773 analytic: -0.067773, relative error: 1.851798e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Training softmax regression classifier using SGD and BGD\n",
      "from algorithms.classifiers import Softmax\n",
      "\n",
      "# # using BGD algorithm\n",
      "# softmax_bgd = Softmax()\n",
      "# tic = time.time()\n",
      "# losses_bgd = softmax_bgd.train(X_train, y_train, method='bgd', batch_size=200, learning_rate=1e-6,\n",
      "#               reg = 1e2, num_iters=1000, verbose=True, vectorized=True)\n",
      "# toc = time.time()\n",
      "# print 'Traning time for BGD with vectorized version is %f \\n' % (toc - tic)\n",
      "\n",
      "# # Compute the accuracy of training data and validation data using Softmax.predict function\n",
      "# y_train_pred_bgd = softmax_bgd.predict(X_train)[0]\n",
      "# print 'Training accuracy: %f' % (np.mean(y_train == y_train_pred_bgd))\n",
      "# y_val_pred_bgd = softmax_bgd.predict(X_val)[0]\n",
      "# print 'Validation accuracy: %f' % (np.mean(y_val == y_val_pred_bgd))\n",
      "\n",
      "# # using SGD algorithm\n",
      "softmax_sgd = Softmax()\n",
      "tic = time.time()\n",
      "losses_sgd = softmax_sgd.train(X_train, y_train, method='sgd', batch_size=200, learning_rate=1e-6,\n",
      "              reg = 1e5, num_iters=1000, verbose=True, vectorized=True)\n",
      "toc = time.time()\n",
      "print 'Traning time for SGD with vectorized version is %f \\n' % (toc - tic)\n",
      "\n",
      "y_train_pred_sgd = softmax_sgd.predict(X_train)[0]\n",
      "print 'Training accuracy: %f' % (np.mean(y_train == y_train_pred_sgd))\n",
      "y_val_pred_sgd = softmax_sgd.predict(X_val)[0]\n",
      "print 'Validation accuracy: %f' % (np.mean(y_val == y_val_pred_sgd))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration 0/1000: loss 1531.789596\n",
        "iteration 100/1000: loss 2.149690"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 200/1000: loss 2.154195"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 300/1000: loss 2.105349"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 400/1000: loss 2.207055"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 500/1000: loss 2.178179"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 600/1000: loss 2.174367"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 700/1000: loss 2.122863"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 800/1000: loss 2.151371"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 900/1000: loss 2.132403"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Traning time for SGD with vectorized version is 10.162502 \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training accuracy: 0.309286"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Validation accuracy: 0.327000\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ggplot import *\n",
      "qplot(xrange(len(losses_sgd)), losses_sgd) + labs(x='Iteration number', y='SGD Loss value')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAq4AAAHtCAYAAADY2/xZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x17bb2c750>"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "<ggplot: (398142557)>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Using validation set to tuen hyperparameters, i.e., learning rate and regularization strength\n",
      "learning_rates = [1e-5, 1e-8]\n",
      "regularization_strengths = [10e2, 10e4]\n",
      "\n",
      "# Result is a dictionary mapping tuples of the form (learning_rate, regularization_strength) \n",
      "# to tuples of the form (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
      "# of data points that are correctly classified.\n",
      "results = {}\n",
      "best_val = -1\n",
      "best_softmax = None\n",
      "# Choose the best hyperparameters by tuning on the validation set\n",
      "i = 0\n",
      "interval = 5\n",
      "for learning_rate in np.linspace(learning_rates[0], learning_rates[1], num=interval):\n",
      "    i += 1\n",
      "    print 'The current iteration is %d/%d' % (i, interval)\n",
      "    for reg in np.linspace(regularization_strengths[0], regularization_strengths[1], num=interval):\n",
      "        softmax = Softmax()\n",
      "        softmax.train(X_train, y_train, method='sgd', batch_size=200, learning_rate=learning_rate,\n",
      "              reg = reg, num_iters=1000, verbose=False, vectorized=True)\n",
      "        y_train_pred = softmax.predict(X_train)[0]\n",
      "        y_val_pred = softmax.predict(X_val)[0]\n",
      "        train_accuracy = np.mean(y_train == y_train_pred)\n",
      "        val_accuracy = np.mean(y_val == y_val_pred)\n",
      "        results[(learning_rate, reg)] = (train_accuracy, val_accuracy)\n",
      "        if val_accuracy > best_val:\n",
      "            best_val = val_accuracy\n",
      "            best_softmax = softmax\n",
      "        else:\n",
      "            pass\n",
      "\n",
      "# Print out the results\n",
      "for learning_rate, reg in sorted(results):\n",
      "    train_accuracy,val_accuracy = results[(learning_rate, reg)]\n",
      "    print 'learning rate %e and regularization %e, \\n \\\n",
      "    the training accuracy is: %f and validation accuracy is: %f.\\n' % (learning_rate, reg, train_accuracy, val_accuracy)\n",
      "           "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The current iteration is 1/5\n",
        "The current iteration is 2/5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The current iteration is 3/5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The current iteration is 4/5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The current iteration is 5/5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "learning rate 1.000000e-08 and regularization 1.000000e+03, \n",
        "     the training accuracy is: 0.134388 and validation accuracy is: 0.123000.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "learning rate 1.000000e-08 and regularization 2.575000e+04, \n",
        "     the training accuracy is: 0.138265 and validation accuracy is: 0.129000.\n",
        "\n",
        "learning rate 1.000000e-08 and regularization 5.050000e+04, \n",
        "     the training accuracy is: 0.159163 and validation accuracy is: 0.162000.\n",
        "\n",
        "learning rate 1.000000e-08 and regularization 7.525000e+04, \n",
        "     the training accuracy is: 0.166184 and validation accuracy is: 0.163000.\n",
        "\n",
        "learning rate 1.000000e-08 and regularization 1.000000e+05, \n",
        "     the training accuracy is: 0.160490 and validation accuracy is: 0.164000.\n",
        "\n",
        "learning rate 2.507500e-06 and regularization 1.000000e+03, \n",
        "     the training accuracy is: 0.395959 and validation accuracy is: 0.395000.\n",
        "\n",
        "learning rate 2.507500e-06 and regularization 2.575000e+04, \n",
        "     the training accuracy is: 0.323469 and validation accuracy is: 0.331000.\n",
        "\n",
        "learning rate 2.507500e-06 and regularization 5.050000e+04, \n",
        "     the training accuracy is: 0.267694 and validation accuracy is: 0.271000.\n",
        "\n",
        "learning rate 2.507500e-06 and regularization 7.525000e+04, \n",
        "     the training accuracy is: 0.275918 and validation accuracy is: 0.289000.\n",
        "\n",
        "learning rate 2.507500e-06 and regularization 1.000000e+05, \n",
        "     the training accuracy is: 0.259388 and validation accuracy is: 0.266000.\n",
        "\n",
        "learning rate 5.005000e-06 and regularization 1.000000e+03, \n",
        "     the training accuracy is: 0.375796 and validation accuracy is: 0.379000.\n",
        "\n",
        "learning rate 5.005000e-06 and regularization 2.575000e+04, \n",
        "     the training accuracy is: 0.232367 and validation accuracy is: 0.228000.\n",
        "\n",
        "learning rate 5.005000e-06 and regularization 5.050000e+04, \n",
        "     the training accuracy is: 0.208449 and validation accuracy is: 0.221000.\n",
        "\n",
        "learning rate 5.005000e-06 and regularization 7.525000e+04, \n",
        "     the training accuracy is: 0.184367 and validation accuracy is: 0.195000.\n",
        "\n",
        "learning rate 5.005000e-06 and regularization 1.000000e+05, \n",
        "     the training accuracy is: 0.154653 and validation accuracy is: 0.146000.\n",
        "\n",
        "learning rate 7.502500e-06 and regularization 1.000000e+03, \n",
        "     the training accuracy is: 0.299490 and validation accuracy is: 0.300000.\n",
        "\n",
        "learning rate 7.502500e-06 and regularization 2.575000e+04, \n",
        "     the training accuracy is: 0.185714 and validation accuracy is: 0.202000.\n",
        "\n",
        "learning rate 7.502500e-06 and regularization 5.050000e+04, \n",
        "     the training accuracy is: 0.189306 and validation accuracy is: 0.181000.\n",
        "\n",
        "learning rate 7.502500e-06 and regularization 7.525000e+04, \n",
        "     the training accuracy is: 0.174510 and validation accuracy is: 0.178000.\n",
        "\n",
        "learning rate 7.502500e-06 and regularization 1.000000e+05, \n",
        "     the training accuracy is: 0.109286 and validation accuracy is: 0.127000.\n",
        "\n",
        "learning rate 1.000000e-05 and regularization 1.000000e+03, \n",
        "     the training accuracy is: 0.302653 and validation accuracy is: 0.313000.\n",
        "\n",
        "learning rate 1.000000e-05 and regularization 2.575000e+04, \n",
        "     the training accuracy is: 0.146429 and validation accuracy is: 0.131000.\n",
        "\n",
        "learning rate 1.000000e-05 and regularization 5.050000e+04, \n",
        "     the training accuracy is: 0.104224 and validation accuracy is: 0.098000.\n",
        "\n",
        "learning rate 1.000000e-05 and regularization 7.525000e+04, \n",
        "     the training accuracy is: 0.095714 and validation accuracy is: 0.101000.\n",
        "\n",
        "learning rate 1.000000e-05 and regularization 1.000000e+05, \n",
        "     the training accuracy is: 0.103265 and validation accuracy is: 0.110000.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Test **best** logistic classifier on test datasets\n",
      "The trained model works well on the test datasets, which has 39.04% accuracy!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_test_predict_result = best_softmax.predict(X_test)\n",
      "y_test_predict = y_test_predict_result[0]\n",
      "test_accuracy = np.mean(y_test == y_test_predict)\n",
      "print 'The test accuracy is: %f' % test_accuracy\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The test accuracy is: 0.383200\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}